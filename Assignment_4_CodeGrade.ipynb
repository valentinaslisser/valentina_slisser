{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ozWCtlhaqQtH"
   },
   "source": [
    "# **P4**: DECISION TREES\n",
    "### Total points: 38\n",
    "\n",
    "Decision trees are a supervised learning algorithm used for both classification and regression tasks. For this assignment we are going to take a look at classification. We can use decision trees for issues where we have numerical but also categorical input and target features. The decision tree algorithm you will be implementing is based off the *ID3* algorithm as described in chapter 3 of the book \"Machine Learning\" by Tom Mitchell, sections **3.1-3.4**.  See Canvas for a link to the PDF.\n",
    "\n",
    "After building our own *ID3* tree, we will then use scikit-learn to explore decision trees with numerical features. Besides working with data containing both categorical and numeric features, this assignment will give us a chance to study the following topics:\n",
    "\n",
    "* Dealing with a more realistic data set. The Iris dataset is useful to get started, as it just works out of the box, but real world datasets will almost always be a lot messier. Having to do some preprocessing to end up with a usable representation is extremely common.\n",
    "* Working with dataframes. `pandas` is a very popular Python library for Data Science that enables you to perform database-like operations on large datasets with great performance. It is a very useful tool to add to your arsenal.\n",
    "* Analysing the results of an algorithm. The results you get when you have (correctly) implemented the algorithm might surprise you. Trying to set up hypotheses about why this is the case and what you could do to improve / prevent / fix this, is a key skill in applying machine learning on real problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTPrd1EdqQtO"
   },
   "source": [
    "# Pandas Warmup\n",
    "\n",
    "Before we start with the actual assignment, we will introduce `pandas` through a set of small exercises. For this we will use the book by the creator of `pandas`: [Python for Data Analysis](https://bedford-computing.co.uk/learning/wp-content/uploads/2015/10/Python-for-Data-Analysis.pdf). You won't have to read the whole book, but it will serve as a useful reference while you figure out how certain operations are done in `pandas`.\n",
    "\n",
    "Below each of the exercises is a set of assertions that test whether you gave the correct answer.  Let's first import Pandas and remove any deprecation warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6GgHWMQDqQtP"
   },
   "outputs": [],
   "source": [
    "# Codegrade Tag0\n",
    "# Ignore deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# import Pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJmtSM7-qQtP"
   },
   "source": [
    "### Series [2 pts]\n",
    "\n",
    "Start by reading Python for Data Analysis pages 111-115 on Series.\n",
    "\n",
    "Create a `Series`-object named `earnings` containing the following *figures* and using the *sources* as its index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36SKHMBuqQtR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sales            39041\n",
      "ads               8702\n",
      "subscriptions    13200\n",
      "donations          292\n",
      "dtype: int64\n",
      "\n",
      "Type of earnings: <class 'pandas.core.series.Series'>\n",
      "\n",
      "Index of earnings: ['sales', 'ads', 'subscriptions', 'donations']\n"
     ]
    }
   ],
   "source": [
    "# Codegrade Tag1\n",
    "earnings_sources = [\"sales\", \"ads\", \"subscriptions\", \"donations\"]\n",
    "earnings_figures = [39041, 8702, 13200, 292]\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "earnings_sources = ['sales', 'ads', 'subscriptions', 'donations']\n",
    "\n",
    "earnings = pd.Series([39041, 8702, 13200, 292], index=earnings_sources)\n",
    "\n",
    "print(earnings)\n",
    "print(\"\\nType of earnings:\", type(earnings))\n",
    "print(\"\\nIndex of earnings:\", list(earnings.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nyGe88e0qQtS"
   },
   "outputs": [],
   "source": [
    "# Codegrade Tag2\n",
    "assert type(earnings) is pd.Series, \"Income has to be a Series\"\n",
    "assert list(earnings.index) == [\"sales\", \"ads\", \"subscriptions\", \"donations\"]\n",
    "assert list(earnings) == [39041, 8702, 13200, 292]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfzojrLxqQtS"
   },
   "source": [
    "Create a `Series`-object named `expenses` using the same format as `earning` with the expenses *figures*. Then create another `Series`-object named `profit` with the profit figures for each category (earnings minus expenses). Lastly, create a variable `total_profit` containing the summed total of `profit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "tVCod4ZHqQtU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expenses Series:\n",
      "ads               4713\n",
      "sales            24282\n",
      "donations            0\n",
      "subscriptions     3302\n",
      "dtype: int64\n",
      "\n",
      "Profit Series:\n",
      "ads               3989\n",
      "donations          292\n",
      "sales            14759\n",
      "subscriptions     9898\n",
      "dtype: int64\n",
      "\n",
      "Total Profit: 28938\n"
     ]
    }
   ],
   "source": [
    "# Codegrade Tag3\n",
    "expenses_sources = [\"ads\", \"sales\", \"donations\", \"subscriptions\"]\n",
    "expenses_figures = [4713, 24282, 0, 3302]\n",
    "\n",
    "expenses_sources = ['ads', 'sales', 'donations', 'subscriptions']\n",
    "\n",
    "expenses = pd.Series([4713, 24282, 0, 3302], index=expenses_sources)\n",
    "\n",
    "profit = earnings - expenses\n",
    "\n",
    "profit_list = list(profit)\n",
    "\n",
    "total_profit = profit.sum()\n",
    "\n",
    "print(\"Expenses Series:\")\n",
    "print(expenses)\n",
    "print(\"\\nProfit Series:\")\n",
    "print(profit)\n",
    "print(\"\\nTotal Profit:\", total_profit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DVGpPikFqQtU"
   },
   "outputs": [],
   "source": [
    "# Codegrade Tag4\n",
    "assert type(expenses) is pd.Series\n",
    "assert type(profit) is pd.Series\n",
    "float(total_profit)\n",
    "assert list(profit) == [3989, 292, 14759, 9898]\n",
    "assert total_profit == 28938"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FyU7mZenqQtV"
   },
   "source": [
    "### DataFrames [3 pts]\n",
    "\n",
    "Next read Python for Data Analysis pages 115-120 on DataFrames.\n",
    "\n",
    "Create a `DataFrame` named `skittles` with the *columns* `amount` and `rating`, using the different colors as the *index*.\n",
    "\n",
    "|&nbsp;      | amount | rating |\n",
    "|------------|--------|--------|\n",
    "| **red**    | 7      | 3      |\n",
    "| **green**  | 4      | 4      |\n",
    "| **blue**   | 6      | 2      |\n",
    "| **purple** | 5      | 4      |\n",
    "| **pink**   | 6      | 3.5    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "4FVKxQ1EqQtV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        amount  rating\n",
      "red          7     3.0\n",
      "green        4     4.0\n",
      "blue         6     2.0\n",
      "purple       5     4.0\n",
      "pink         6     3.5\n",
      "\n",
      "Red amount: 7\n",
      "Blue rating: 2.0\n"
     ]
    }
   ],
   "source": [
    "# Codegrade Tag5\n",
    "from pandas import DataFrame\n",
    "\n",
    "skittles = DataFrame({\n",
    "    'amount': [7, 4, 6, 5, 6],\n",
    "    'rating': [3, 4, 2, 4, 3.5]\n",
    "}, index=['red', 'green', 'blue', 'purple', 'pink'])\n",
    "\n",
    "print(skittles)\n",
    "\n",
    "print(\"\\nRed amount:\", skittles.loc['red', 'amount'])\n",
    "print(\"Blue rating:\", skittles.loc['blue', 'rating'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "yep7MfKWqQtW"
   },
   "outputs": [],
   "source": [
    "# Codegrade Tag6\n",
    "assert type(skittles) is DataFrame\n",
    "assert list(skittles.index) == [\"red\", \"green\", \"blue\", \"purple\", \"pink\"]\n",
    "assert list(skittles.columns) == [\"amount\", \"rating\"]\n",
    "assert skittles.loc[\"red\", \"amount\"] == 7\n",
    "assert skittles.loc[\"blue\", \"rating\"] == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gMHfpiBqQtW"
   },
   "source": [
    "Calculate the average _rating_ of all the skittles and store it in a variable called `skittles_average`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "JRB2T644qQtX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skittles Average Rating: 3.3\n",
      "\n",
      "Updated Skittles DataFrame:\n",
      "        amount  rating  score\n",
      "red          7     3.0   21.0\n",
      "green        4     4.0   16.0\n",
      "blue         6     2.0   12.0\n",
      "purple       5     4.0   20.0\n",
      "pink         6     3.5   21.0\n"
     ]
    }
   ],
   "source": [
    "skittles_average = skittles['rating'].mean()\n",
    "\n",
    "print(\"Skittles Average Rating:\", skittles_average)\n",
    "\n",
    "skittles['score'] = skittles['amount'] * skittles['rating']\n",
    "\n",
    "print(\"\\nUpdated Skittles DataFrame:\")\n",
    "print(skittles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "NYnGFGEUqQtX"
   },
   "outputs": [],
   "source": [
    "# Codegrade Tag8\n",
    "float(skittles_average)\n",
    "assert skittles_average == 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Si34yQ_bqQtX"
   },
   "source": [
    "Add a new column to the skittles `DataFrame` called `score`. The score of a color is equal to `amount * rating`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "UhbGaeSwqQtY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        amount  rating  score\n",
      "red          7     3.0   21.0\n",
      "green        4     4.0   16.0\n",
      "blue         6     2.0   12.0\n",
      "purple       5     4.0   20.0\n",
      "pink         6     3.5   21.0\n",
      "\n",
      "'score' in skittles columns: True\n"
     ]
    }
   ],
   "source": [
    "skittles['score'] = skittles['amount'] * skittles['rating']\n",
    "\n",
    "print(skittles)\n",
    "\n",
    "print(\"\\n'score' in skittles columns:\", \"score\" in skittles.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "QIUDjOO4qQtY"
   },
   "outputs": [],
   "source": [
    "# Codegrade Tag10\n",
    "assert \"score\" in skittles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kITap79SqQtY"
   },
   "source": [
    "### (Re)Indexing [2 pts]\n",
    "\n",
    "Read Python for Data Analysis pages 122-128, Reindexing, Dropping entries, and \"Indexing, selection, and filtering\".\n",
    "\n",
    "Reindex the given `DataFrame` for columns 'a', 'c', and 'e', using indices 10, 20, 50, 60 and store the result in the same `frame` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "8TeJXpXlqQtY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       a     b     c     d     e     f     g\n",
      "10   0.0   1.0   2.0   3.0   4.0   5.0   6.0\n",
      "20   7.0   8.0   9.0  10.0  11.0  12.0  13.0\n",
      "30  14.0  15.0  16.0  17.0  18.0  19.0  20.0\n",
      "40  21.0  22.0  23.0  24.0  25.0  26.0  27.0\n",
      "50  28.0  29.0  30.0  31.0  32.0  33.0  34.0\n",
      "60  35.0  36.0  37.0  38.0  39.0  40.0  41.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "frame = DataFrame(\n",
    "    np.arange(6 * 7.0).reshape((6, 7)),  \n",
    "    index=[10, 20, 30, 40, 50, 60],       \n",
    "    columns=list('abcdefg')              \n",
    ")\n",
    "\n",
    "print(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "JZ5PxwwgqQtZ"
   },
   "outputs": [],
   "source": [
    "# Codegrade Tag12\n",
    "assert type(frame) == DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFGjN0nXqQtZ"
   },
   "source": [
    "Replace all values in the data frame that are *divisible by 3* with the value *0*, and once again store the result in the `frame` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xn-4hw0aqQtZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       a     b     c     d     e     f     g\n",
      "10   0.0   1.0   2.0   0.0   4.0   5.0   0.0\n",
      "20   7.0   8.0   0.0  10.0  11.0   0.0  13.0\n",
      "30  14.0   0.0  16.0  17.0   0.0  19.0  20.0\n",
      "40   0.0  22.0  23.0   0.0  25.0  26.0   0.0\n",
      "50  28.0  29.0   0.0  31.0  32.0   0.0  34.0\n",
      "60  35.0   0.0  37.0  38.0   0.0  40.0  41.0\n"
     ]
    }
   ],
   "source": [
    "frame = DataFrame(np.where(frame % 3 == 0, 0, frame), \n",
    "                  index=frame.index, \n",
    "                  columns=frame.columns)\n",
    "\n",
    "print(frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZLgvHSzxqQta"
   },
   "outputs": [],
   "source": [
    "# Codegrade Tag14\n",
    "assert type(frame) == DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded processed.cleveland.data\n",
      "Successfully downloaded processed.hungarian.data\n",
      "Successfully downloaded processed.switzerland.data\n",
      "Successfully downloaded processed.va.data\n",
      "\n",
      "Files in current directory:\n",
      "['Assignment_2_CodeGrade_V2-4.ipynb', 'IMG_7353.CR2', 'week1-1.pdf', 'a3_files', 'BSML_2023_Practice_Exam_Midterm.pdf', 'Mock_Exam_Practice_2.pdf', 'IMG_7623.CR2', 'Discord.dmg', 'achterkantpaspoort.HEIC', 'ISOC-History-of-the-Internet_1997-3.pdf', 'CO_werkcollege8.pdf', 'Homework1_14955793_15615901_E.ipynb', 'googlechrome.dmg', 'LA8_GramSchmidt_LSQ.pdf', 'IMG_7743.CR2', 'WC8_student.pdf', 'IMG_9839.PNG', '1-s2.0-S0167739X02000687-main.pdf', 'IMG_2083.HEIC', 'Homework<assignment-number>_<St>_<14955792>_<GroupName>.ipynb', 'dsa_lec09.pdf', 'timetable_2024-09-02-2.ics', 'Assignment_2_CodeGrade_V2.ipynb', 'IMG_7596.CR2', 'IMG_7582.CR2', 'IMG_7583.CR2', 'IMG_9794 2.PNG', 'fitch', 'background1.jpeg', 'BNs_student-2.ipynb', 'world-happiness-report-2021.csv.xls', 'timetable_2024-09-02-3.ics', 'Transparency-and-explainability-of-AI-systems--Fro_2023_Information-and-Soft.pdf', 'Address List -                                                                                 Internationaal en Europees recht | Stoomtraining Groep 1 - 2025-03-28T15_00_20.xlsx', 'extra_exercises_week3.pdf', 'IMG_9838.jpg', 'Onderzoeksvoorstel.pdf', 'mst.pdf', '240326_academische-kalender-2024-2025-nl.pdf', 'IMG_7742.CR2', 'Mitchell-Ch3.pdf', 'HW2_VS-26.ipynb', 'Week6_Extra_Exercises_SlideStyle_Math.pdf', 'wc8.pdf', 'hw2_vs-5.py', 'IMG_9794.PNG', 'ISOC-History-of-the-Internet_1997-2.pdf', 'IMG_7636.CR2', 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'IMG_9860.HEIC', 'Valentina Slisser.pdf', 'HW2_VS-5.ipynb', 'IMG_7352.CR2', 'IMG_7420.CR2', 'tttv2025-practice-midterm.pdf', 'homework5b.prolog', 'BNs.pdf', 'homework6-1.tex.html', 'IMG_7378.CR2', 'IMG_7350.CR2', 'IMG_7422.CR2', 'tttv2025-lab-4.ipynb', 'Voorbereidende opdracht ALF 2.2 wg 1 (Analyse samenwerkingsvaardigheden).docx', 'HW2_VS-19.ipynb', 'week1-2.pdf', 'IMG_1309.jpg', 'Week 6 - Judicial protection against the Member States - Canvas.pdf', 'homework5.tex', 'NPLEIJUAU-BI22001_Inkijkexemplaar_Leidraad_voor_Juridische_auteurs.pdf', 'LA6_subspaces_bases_coordinates.pdf', 'HW2_VS-7.ipynb', 'Answer_WC7.pdf', 'midterm-cheat.pdf', 'IMG_7783.CR2', 'HW2_VS-24.ipynb', 'homework2.pl', 'IMG_7542.CR2', 'animal_game.py', 'hwk1-1.tex.html', 'Dataverzameling Week 1.xlsx', 'ExactInference_student(2)-2.ipynb', 'IMG_7782.CR2', 'Hanna Feij.pdf', 'tttv2025_hw4_theory.pdf', 'CM.com - General Admission - XL77104407.pdf', 'Assignment_1_CodeGrade_VS2-2.ipynb', 'Answer_WC6.pdf', 'wc1a.pdf', 'week3-1.pdf', 'homework5.tex.html', 'IMG_7621.CR2', 'IMG_7635.CR2', 'homework4.tex.html', 'Technisch Rapport Latex.zip', 'multipleChoiceResponses.csv', 'IMG_7386.CR2', 'homework4.tex', 'IMG_7392.CR2', 'IMG_1308.jpg', 'Assignment_1_CodeGrade_V2.ipynb - Colab.html', 'IMG_7423.CR2', 'web.py', 'IMG_7351.CR2', 'IMG_7437.CR2', 'IMG_7427.CR2', 'IMG_7396.CR2', 'ISOC-History-of-the-Internet_1997-5.pdf', 'IMG_7631.CR2', 'IMG_7625.CR2', 'homework6.prolog', 'IMG_9793.PNG', 'IMG_7802.CR2', 'IMG_9787.PNG', 'Assignment_1_Completed.ipynb', 'homework5a-2.prolog', 'derivative_rules_guide.md', 'IMG_7779.CR2', 'RL example.r', 'ICP 2025 - HC1 - Brein.pptx', '.DS_Store', '800157.805047.pdf', 'Assignment_2_CodeGrade_V2-2.ipynb', '37852_UntypedDataSet_05062025_141939.csv', '800157.805047-3.pdf', 'Practice for Midterm.pdf', 'Assignment_1_CodeGrade_Valentina_Slisser-2.ipynb', '800157.805047-2.pdf', 'HW2_VS-3.ipynb', 'extra_exercises_week5.pdf', 'IMG_7778.CR2', 'Side-by-Side.pdf', 'IMG_7803.CR2', 'IMG_9792.PNG', 'IMG_7817.CR2', 'HW2_VS-20.ipynb', 'ISOC-History-of-the-Internet_1997-4.pdf', 'oefentraining her privaat.pdf', 'IMG_8329.JPG', 'BNs_student-4.ipynb', 'tttv2025_hw1_theory.pdf', 'IMG_9792 2.PNG', 'IMG_7397.CR2', 'IMG_7340.CR2', 'IMG_7426.CR2', 'IMG_7354.CR2', 'IMG_7356.CR2', 'IMG_7430.CR2', 'HW2_VS-22.ipynb', 'WhatsApp-2.25.7.77.dmg', 'Voorbereidende opdracht ALF 2.1 wg 9 (Peerfeedback paper).docx', 'DF7CCBCF-71FB-427F-AAAE-EC95F5373DA5.jpg', 'Firefox 131.0.3.dmg', 'IMG_7395.CR2', 'timetable_2024-09-02.ics', 'IMG_7626.CR2', 'BSML_2023_Midterm_Reference_Sheet.pdf', 'IMG_9861.HEIC', 'Optimalisatie.pdf', 'Leren_week2_werkcollege.pptx', 'freeFormResponses.csv', 'formules.py', 'IMG_7801.CR2', '115-Modernerrorcorrectingcodesfor4Gandbeyond-2.pdf', 'paths.py', 'IMG_7746.CR2', 'gradient_descent_14955792t.ipynb', 'IMG_7785.CR2', 'Week 3 - Free Movement of Persons and EU Citizenship - Canvas.pdf', 'W5-self-study-exercises-solutions.pdf', 'tttv2025-lab-2.ipynb', 'hand en wereldbol.jpeg', 'qtreenotes.tex', 'Exam-like 1 with solutions.pdf', 'Samenwerkingsprotocol Amsterdam Law Firm.docx', 'mosaic 1.jpeg', 'IMG_2082.HEIC', 'extra_exercises_week6.pdf', 'SCP+rapport+Digitaal+vervlochten%2C+maar+ook+verbonden.pdf', 'overzicht vaste lasten.pdf', 'hwk1_PS.tex', 'LA7_Orthogonality.pdf', 'WhatsApp Image 2025-01-13 at 11.58.31 (1).jpeg', 'Homework4_15615901_14955792_E2.pdf', '.localized', 'Assignment_1_CodeGrade_V2.ipynb', 'huiswerk_calculus_week_2-2.pdf', 'IMG_7800.CR2', '3F430323-C258-4EC9-84A6-8D3C7311C144.jpg', 'hw2_vs-4.py', 'Untitled document.pdf', 'IMG_7394.CR2', 'WhatsApp Image 2025-01-27 at 11.59.37.jpeg', 'phpMyAdmin-5.2.1-all-languages', 'cad95a8a-04dd-4624-8e63-aa8918b4782e.JPG', 'HW2_VS-25.ipynb', 'Stoomtraining Europees Recht (2025-2026).pdf', 'IMG_7330.CR2', 'hwk1_PS.pdf', 'true-2.html', 'IMG_7668.CR2', 'ICP Werkcollege Week 6 - Groepsopdracht.docx', 'rainforest2.jpeg', 'rivier.jpeg', 'puck 21', '2025_26_Theory_Solutions_10.pdf', 'week5.ipynb', 'Exam-like 3 - with solutions.pdf', 'course0.pdf', 'myfitch.sty.tex', 'DatastructurenEnAlgoritmen_Final_11_11_2024.pdf', 'IMG_7720.CR2', 'sort_linked_list-2.py', 'treingraaf_stations.json', 'HW2_VS-6.ipynb', 'Sophia Dokter.docx', 'tttv2025-lab-5.ipynb', 'IMG_7522.CR2', 'HW2_VS-18.ipynb', 'IMG_2089.HEIC', 'dsa06.pdf', 'timetable_2024-11-04.ics', 'dsa07.pdf', 'dsa13.pdf', 'IMG_7523.CR2', 'conversions.py', 'exercises_week3_solutions.pdf', 'w2-self-study-exercises-solutions.pdf', 'brown ombre 1.jpeg', 'week2-1.pdf', 'IMG_1689.HEIC', 'voorkantpaspoort.HEIC', 'course1.pdf', 'Spread of Disease Network.nlogo', 'KI22-boeken-jaar1', 'pss-hw-5b-solution.prolog', 'IMG_7641.CR2', 'Afgeleiden.pdf', 'Week 5 - EU competition law .pdf', '216172782415167021.pdf', 'Silicon.Mac.Pulsar-1.122.0-arm64.dmg', 'IMG_7455.CR2', 'IMG_7441.CR2', 'ICP 2025 - HC5 - Besluitvorming.pptx', 'Overzicht opdrachten herkansing blok 6 20242025.docx', 'VALENTINA_SLISSER_HW6.pdf', 'procedure document .pdf', 'IMG_7657.CR2', 'IMG_1342.PNG', 'course3.pdf', 'Week5_Exercises_1_2_3_6_Solutions_SlideStyle.pdf', 'owid-covid-data.csv', 'W5-self-study-exercises.pdf', 'huiswerk4_2019.pdf', 'Machine Learning - Week 1.pdf', 'Opdracht wg 2 (Professionele juridische identiteit I).docx', 'Werkcollege Opdrachten week 1.pdf', 'oefenvragen P&F.pdf', 'IMG_2085.HEIC', 'dsa05.pdf', 'main.tex', 'HW2_VS-4.ipynb', 'dsa04.pdf', 'fotos', 'IMG_7520.CR2', 'penguins-2.csv', 'Voorblad Verbintenissenrecht 20-12-2024.docx', 'week2-2.pdf', 'IMG_1685.HEIC', 'exercises week 2 (with solutions).pdf', 'IMG_7736.CR2', 'sort_array-2.py', 'IMG_7722.CR2', 'WC7_student.pdf', 'course2.pdf', 'dictionary_demo.py', 'IMG_7642.CR2', 'w1-self-study-exercises.pdf', 'ICP 2025 - HC2 - Perceptie en Aandacht.pptx', 'pss-hw-5.pdf', 'Sponsorpropositie.pdf', 'BNs_student-3.ipynb', 'IMG_9098.HEIC', 'tttv2025-lab-3.ipynb', 'IMG_7444.CR2', 'IMG_1016.HEIC', 'Time_Complexity_Samenvatting.pdf', 'IMG_7478.CR2', 'HC10 - H14.IndividueleVerschillen_MR2025.pdf', 'IMG_1347.PNG', 'weather.py', 'quiz1.pdf', 'InfoVis 2025 - assignment 1 description.docx', 'IMG_7726.CR2', 'Week5_Week6_Solutions_DOCUMENT.pdf', '201702AasHW.pdf', 'progHW3-student-2.ipynb', 'homework4.pdf', 'Voorbeeld_Voorbereidende_Rapport_Keerpunten_in_de_Informatiewetenschappen.pdf', 'tttv2025_hw3_theory.pdf', 'Ivanka van de Velden_bezwaar.pdf', 'IMG_9848.jpg', 'HW2_VS-23.ipynb', 'rufus-4.5.exe', '2025_26_Theory_Solutions_9.pdf', '2025_26_Theory_Solutions_8.pdf', 'dsa01.pdf', 'Assignment_4_CodeGrade.ipynb', 'IMG_2084.HEIC', 'IMG_9849.jpg', '2024_25_DSA_KI_Assignment_3.pdf', 'homework5.pdf', 'weg.jpeg', 'Introduction-2.pdf', 'Print grading scheme · BSML Final Exam 2024.pdf', 'IMG_7727.CR2', 'Week 1 - Regulating the Internal Market.pdf', 'IMG_7690.CR2', 'UVA-FDR-0259.pdf', 'IMG_7653.CR2', 'IMG_7486.CR2', 'IMG_7479.CR2', 'Time_Complexity_Samenvatting_Uitgebreid.pdf', 'IMG_7337.CR2', 'Assignment_2_CodeGrade_V2-3.ipynb', 'Week 5 - EU criminal and asylum law .pdf', 'IMG_1350.jpg', 'IMG_7484.CR2', 'IMG_1344.jpg', 'Assignment_3_CodeGrade_V2-2.ipynb', 'pav', 'IMG_7645.CR2', 'nl-en-dict.txt', 'data_description.txt', 'InfoVis 2025 - data story proposal template.docx', 'progHW1-student.ipynb', '__pycache__', 'HW-2.tex.html', 'IMG_7725.CR2', 'IMG_1688.HEIC', '13.pdf', 'Leren 2021 Midterm - Solutions.pdf', 'Lecture Notes week 1.pdf', 'entropy-8.pdf', 'IMG_7296.CR2', 'C&O-2018-2019-deel2-example.pdf', 'dsa03.pdf', 'dsa02.pdf', 'IMG_2088.HEIC', 'BNs_student-5.ipynb', 'ResolutieNew.pdf', 'IMG_9793 2.PNG', 'In-class 1 with solutions.pdf', 'homework6.pdf', '84888NED-202409030630', 'IMG_7718.CR2', 'HW2_VS-21.ipynb', 'true-3.html', 'Assignment_1_CodeGrade_VS2.ipynb', 'tttv2025-lab-1.ipynb', 'IMG_7644.CR2', 'IMG_1345.jpg', 'Assignment_1_CodeGrade_Valentina_Slisser.ipynb', 'HW2_VS-2.ipynb', 'IMG_7485.CR2', 'sort_linked_list.py', 'InfoVis 2025 - assignment 1 grading rubric.pdf', 'IMG_7334.CR2', 'curriculum_schema_kunstmatige-intelligentie_2024-2025.pdf', '1b3ebd71-e2cc-49da-a3a4-270f9bda26d3_Original.jpg', 'IMG_7339.CR2', 'IMG_7311.CR2', 'homework6-1.tex', 'HW2_VS-10.ipynb', 'IMG_1348.jpg', 'BNs-student.ipynb.html', 'IMG_7488.CR2', 'Install Spotify.app', 'UVA-FDR-0257.pdf', 'colloquim 1 Valentina Slisser.pdf', '201903A.pdf', 'In-class 3 - with solutions.pdf', 'Denkgereedschap voor de bezwaarjurist.pdf', 'CM.com - General Admission - XL77104407-2.pdf', 'huiswerk6.pdf', 'IMG_9847.jpg', 'entropy-4.pdf', 'IMG_9853.jpg', 'midterm_exam_solutions.pdf', '85454NED_UntypedDataSet_05062025_142042.csv', 'true.html', '2025_26_Theory_Solutions_6.pdf', '2025_26_Theory_Solutions_7.pdf', 'webinar-appointment.ics', 'W7-self-study-exercises.pdf', 'hwk3_bsml.pdf', 'ExactInference-student(2).ipynb', 'IMG_9846.jpg', 'entropy-5.pdf', 'Scan  Akkermans_ van Melle[64].pdf', '216172782415167021-2.pdf', 'Assignment_1_CodeGrade_V2-3.ipynb', 'HitPaw FotorPea Installer.app', 'IMG_9852.PNG', 'Week7_SKB.pdf', 'CM.com - General Admission - XL77104407-3.pdf', 'hwk2-2.pdf', 'IMG_7728.CR2', '2025_2026_DSA_KI_Assignment_2.pdf', 'In-class 1.pdf', 'IMG_7648.CR2', 'sort_array.py', 'IMG_7489.CR2', 'IMG_1349.jpg', 'label-10062025172733_0_4424721_.pdf', 'IMG_7338.CR2', 'IMG_7448.CR2', 'IMG_7306.CR2', 'Meervoudige_integralen.pdf', 'IMG_9864.HEIC', 'Turning Points PSMW-final.pdf', 'IMG_7845.CR2', 'IMG_1691.HEIC', 'In-class 3.pdf', 'HW-2.tex', 'intro_numpy_pyplot_LA_student.ipynb', 'IMG_7716.CR2', 'IMG_1687.HEIC', 'homework5b-2.prolog', 'IMG_9850.PNG', 'IMG_7528.CR2', 'ICP 2025 - HC11 - Taal.pptx', 'entropy-7.pdf', 'IMG_9844.jpg', 'IMG_7514.CR2', 'NetLogo-6.4.0.dmg', 'Reparatie - Beschouwing.docx', '2025_26_Theory_Solutions_5.pdf', 'processed.cleveland.data', 'preadvies.pdf', '2025_26_Theory_Solutions_4.pdf', 'IMG_7298.CR2', 'entropy-6.pdf', 'IMG_7515.CR2', 'plantingtree', 'IMG_9845.jpg', 'huiswerk4.pdf', 'IMG_2087.HEIC', 'Exercise5_PredictivePosterior_TrafficLights_Solution.pdf', 'IMG_9851.PNG', 'IMG_4436_Original.jpg', 'IMG_7717.CR2', 'Leren 2021 Midterm.pdf', 'HW-3.tex', '2025_2026_DSA_KI_Assignment_1.pdf', 'IMG_7944.HEIC', 'IMG_7844.CR2', 'In-class 2.pdf', 'Werkcollegeopdracht - week 1.docx', 'compressjpeg.zip', 'IMG_7663.CR2', 'HW2_VS-12.ipynb', 'IMG_7677.CR2', 'MockExam (labeled).pdf', 'Week 7 - Judicial protection against the EU institutions - Canvas.pdf', 'IMG_2091.HEIC', 'Week 2 - EU Legal Harmonization  .pdf', 'processed.switzerland.data', 'slides_inleiding_logica_college_3.pdf', 'IMG_7459.CR2', 'IMG_2090.HEIC', 'concept versie paper.pdf', 'Technisch Rapport Latex 3', 'les1.pdf', 'IMG_7673.CR2', 'IMG_7667.CR2', 'InfoVis 2025 - data story registration template 2.pdf', 'IMG_7945.HEIC', 'HW2_VS-8.ipynb', 'lai-copy-copy.pdf', 'IMG_9855.jpg', 'IMG_9841.jpg', 'HW2_VS-16.ipynb', 'entropy-2.pdf', 'IMG_2086.HEIC', 'IMG_7539.CR2', 'hwk-week6.pdf', 'MNs.pdf', 'hw2_vs.py', 'huiswerk_calculus_5-2.pdf', 'dsa08.pdf', 'Homework<2>_<15615901>_<14955792>_<GroupName>.ipynb', '2025_26_Theory_Solutions_1.pdf', 'ubuntu-24.04-desktop-amd64.iso', 'voorbeeld oefentraining.pdf', 'IMG_9840.jpg', 'entropy-3.pdf', 'Slide_Exercise2AB.png', 'IMG_1686.HEIC', 'Template target sheets (download)', 'IMG_9854.PNG', 'booklist.py', 'Functies+.pdf', 'hwk1-1.tex', 'IMG_7855.CR2', 'IMG_1690.HEIC', 'C&O-2018-2019-deel2.pdf', 'swipl-9.2.6-1.fat.dmg', 'Week6_Exercise5_PredictivePosterior.png', 'IMG_9865.HEIC', 'IMG_7672.CR2', 'Week 6 opdracht vragen.pdf', 'progHW3-student.ipynb', 'IMG_7464.CR2', '160211708.html', 'wc7-1.pdf', 'IMG_7314.CR2', 'IMG_7328.CR2', 'KI22-boeken-jaar1.zip', 'IMG_7664.CR2', 'IMG_7670.CR2', 'IMG_7658.CR2', 'IMG_7843.CR2', 'Wetsvoorstel Drijvende opstallen - een juridisch fundament voor bouwen op water - deel II-2.pdf', 'treingraaf_links.json', 'PAV schrijfopdracht-1.pdf', 'tcs-homework-1.pdf', 'HW2_VS.ipynb', 'IMG_7738.CR2', 'Zoom.pkg', 'IMG_9842.jpg', 'hwk-week5.pdf', 'Technisch Rapport Latex 2', 'Week6_SKB.pdf', 'InfoVis 2025 - data story project description.pdf', '492651409.html', 'Technisch Rapport Latex', 'regression-LA-updated-student (1).ipynb', 'Dataverzameling Week 1.pdf', 'Print grading scheme · Practice final exam.pdf', 'IMG_9843.PNG', 'Werkgroepcasus week 1-24.docx', 'wetenschappelijke-integriteit-in-de-rechtenopleiding.pdf', 'Opdracht ALF 2.1 wg 10 (Logboek Rechtswetenschappelijk onderzoek doen).docx', 'Integreren.pdf', 'IMG_7739.CR2', 'Print questions · SKB Homework 7.pdf', 'IMG_7705.CR2', 'Wetenschappelijke schrijfopdracht PAV.pdf', 'IMG_7711.CR2', 'HW2_VS-14.ipynb', 'IMG_7856.CR2', 'huiswerk_calculus_5.pdf', 'huiswerk week 2 keerpunten-1.pdf', 'world-happiness-report.csv.xls', 'boos.pl', 'swipl-9.2.6-1.x86_64.dmg', 'homework5a.prolog', 'IMG_7498.CR2', 'PAV schrijfopdracht.pdf', 'tttv2025_hw5_theory.pdf', 'IMG_7301.CR2', 'IMG_7400.CR2', 'NIPS-2017-attention-is-all-you-need-Paper-3.pdf', 'IMG_9862.HEIC', 'IMG_7616.CR2', 'IMG_7825.CR2', 'IMG_9213.jpg', 'cheat_sheet_final.pdf', 'MNs_student.ipynb', 'a3_files 2', 'Untitled document-2.pdf', 'Untitled document-3.pdf', 'ExactInference_student(2).ipynb', 'Schrijfopdracht Versie 1 - David Snoeks en Ryan Rodrigus.pdf', 'IMG_9858.HEIC', 'IMG_7561.CR2', 'IMG_7575.CR2', 'IMG_7549.CR2', 'IMG_7575.psd', 'Metadata document .pdf', 'Slack-4.44.65-macOS.dmg', 'IMG_7788.CR2', 'Spiekbrief_Bayes_Uniform.pdf', 'huiswerk_calculus_week_1.pdf', 'case-studies.pdf', 'IMG_7763.CR2', 'HW2_VS-13.ipynb', 'exercises week2.pdf', 'IMG_7830.CR2', 'IMG_7824.CR2', 'gradient_descent_student.ipynb', 'IMG_7603.CR2', 'Unknown', 'NIPS-2017-attention-is-all-you-need-Paper-2.pdf', 'IMG_7415.CR2', 'IMG_7373.CR2', 'IMG_7401.CR2', 'pss-hw-4-solution.prolog', 'hw2_vs-3.py', 'lecture_notes_week6.pdf', 'hwk1-1.pdf', 'MAMP & MAMP PRO Downloader.app', 'Assignment_1_student.ipynb', 'Eindproject-Detailbeschrijving-2.pdf', 'calculus_week_7.pdf', 'IMG_7832.CR2', 'wc3.pdf', 'In-class 2 - with solutions.pdf', 'Slide_Exercise2D.png', 'Samenvatting artikel - Bijeenkomst 5.5.pdf', 'a3_files-1.zip', 'regression-LA-updated-student.ipynb', 'IMG_6587_Original.jpg', 'HW2_VS-11.ipynb', 'zelfcheckbonus week 1 vraag 1.HEIC', 'Concept paper.pdf', 'processed.hungarian.data', 'BNs_student.ipynb', 'IMG_9787 2.PNG', 'catalogue', 'IMG_7774.CR2', 'huiswerk_calculus_week_2.pdf', 'assignment1_datavisualisation.pdf', 'wc2.pdf', 'Wetsvoorstel Drijvende opstallen - een juridisch fundament voor bouwen op water - deel II.pdf', 'IMG_7600.CR2', 'Achtergronddocument+1+Analyse+databestand+WoON+%28aangepaste+versie+januari+2025%29.pdf', 'ssrn-4997519.pdf', 'IMG_1315.jpg', 'pgms-glossary.pdf', 'IMG_7980.JPG', 'WhatsApp Image 2025-02-25 at 13.04.00.jpeg', 'IMG_7370.CR2', 'IMG_7358.CR2', 'lecture_notes_week7.pdf', 'Assignment_1_CodeGrade_V2-2.ipynb', 'IMG_7412.CR2', 'IMG_7374.CR2', 'Leren_week1_werkcollege.pptx', 'lecture_notes_week3.pdf', 'pss-hw-5a-solution.prolog', 'IMG_1311.jpg', 'bsml-06-01-model-selection.pdf', 'TTTV2025_lecture_1_compact.pdf', 'CO_werkcollege7.pdf', 'IMG_7959.HEIC', 'huiswerk_calculus_week_6.pdf', 'IMG_7770.CR2', '978-3-031-39101-9.pdf', 'F.pdf', 'NeurIPS-2022-visual-correspondence-based-explanations-improve-ai-robustness-and-human-ai-team-accuracy-Paper-Conference.pdf', 'wc6-1.pdf', 'week1 - solutions.pdf', 'W6-self-study-exercises.pdf', 'AA20220153.pdf', 'dsa_lec12.pdf', 'Untitled document-4.pdf', 'IMG_1051_Original.jpg', 'WC6_student.pdf', 'The Transformer_ Attention Is All You Need.pdf', 'Zoom-3.pkg', 'Zoom-2.pkg', 'introduction.pdf', 'Untitled document-5.pdf', 'IMG_9837.jpg', 'Sophia Dokter (1).docx', 'Homework<1>_<1495592>_<15615901>_<E>.pdf', 'entropy.pdf', 'IMG_7759.CR2', 'indexer', 'IMG_7765.CR2', 'hwk2_PS.pdf', 'CO_werkcollege6.pdf', 'IMG_7639.CR2', 'image0.jpg', 'HW2_VS-15.ipynb', 'lec1-3.pdf', 'IMG_1310.jpg', 'Exercises5_6_Solutions_SlideStyle.pdf', 'Week5_SKB.pdf', 'Assignment_3_CodeGrade_V2.ipynb', 'lecture_notes_week2.pdf', 'w2-self-study-exercises.pdf', 'IMG_7349.CR2', 'IMG_7407.CR2', 'IMG_7413.CR2', 'Onderzoeksvoorstel-2.pdf', 'Mock_Exam_Answers.pdf', 'Slide_Exercise1.png', 'IMG_7405.CR2', 'W6-self-study-exercises-solutions.pdf', 'IMG_7363.CR2', 'HW2_VS-17.ipynb', 'IMG_7439.CR2', 'Ivanka van de Velden_bezwaar (1).pdf', 'IMG_7388.CR2', 'IMG_1689.png', 'HW2_VS-9.ipynb', 'hwk3_bsml.tex', 'wc5.pdf', 'hwk2-1.tex.html', 'Answer_WC8.pdf', 'Practice for Midterm - with solutions.pdf', 'IMG_8779_Original.jpg', 'concept versie paper-2.pdf', 'Bijlagen+SCP+rapport+Digitaal+vervlochten%2C+maar+ook+verbonden.pdf', 'homework1.pl', 'Presentatie opdracht.docx', 'InfoVis 2025 - data story registration template.docx', 'IMG_9859.HEIC', 'submission_template.tex', 'Exam-like 1.pdf', 'ISOC-History-of-the-Internet_1997.pdf', 'Huiswerk2_Valentina_Slisser_14955792.pdf', 'dsa_lec10.pdf', 'Practice final exam.pdf', 'Tentamen Verbintenissenrecht 2024-2025 eerste kans 20-12-2024 canvas.docx', 'penguins.csv', 'Slide_Exercise2C.png', 'IMG_7766.CR2', 'wc4.pdf', 'IMG_7821.CR2', 'IMG_9863.HEIC', 'SNIC2024-HYXZN-1-pdf.pdf', 'processed.va.data', 'IMG_7389.CR2', '2023-pss-final_answers.pdf', 'IMG_1313.jpg', 'hw2_vs-2.py', 'Limieten.pdf']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# Base URL for the datasets\n",
    "base_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/'\n",
    "\n",
    "# List of dataset files\n",
    "file_names = [\n",
    "    'processed.cleveland.data',\n",
    "    'processed.hungarian.data',\n",
    "    'processed.switzerland.data',\n",
    "    'processed.va.data'\n",
    "]\n",
    "\n",
    "# Download each file\n",
    "for filename in file_names:\n",
    "    url = base_url + filename\n",
    "    try:\n",
    "        # Send a GET request to download the file\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Write the content to a local file\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Successfully downloaded {filename}\")\n",
    "        else:\n",
    "            print(f\"Failed to download {filename}. Status code: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {filename}: {e}\")\n",
    "\n",
    "# Verify the files were downloaded\n",
    "print(\"\\nFiles in current directory:\")\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4_UQRUQqQta"
   },
   "source": [
    "# Predicting heart disease\n",
    "\n",
    "The data set we will be using for this assignment contains heart disease diagnosis results from 4 different hospitals. The data set can be found [here](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/) (NB: the link may not seem to work, but it does if you use pandas to load it directly). Load the dataset.\n",
    "\n",
    "Lets start by looking at the [heart-disease.names](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/heart-disease.names) file, which contains a description of the data set. The file also gives an explanation for the values of the different variables, so when our tree is complete we can interpret the decision rules created by the algorithm.\n",
    "\n",
    "Some variables included here, like *#9 cp: chest pain type*, with 4 labels for different types of chest pain, are clearly categorical. Then there are variables like *#12 chol: serum cholestoral in mg/dl*, containing the concentration of cholesterol, an obvious numeric value. The ability to handle both of these types of data is something not many other machine learning algorithms can do effectively, so in theory a decision tree should be perfect for this data.\n",
    "\n",
    "### Taking a first look [2 pts]\n",
    "\n",
    "Start by loading the four `processed.X.data` files into a *Pandas DataFrame* with the function `pd.read_csv`. Do not forget to manually set the names of the columns! You can find these names listed in the [heart-disease.names](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/heart-disease.names) file.\n",
    "\n",
    "Create a list `data` that contains these four dataframes and print the dataset. You should see a couple of unexpected values pop up, which probably indicate a missing value. The file describing the data sets states that missing values are indicated by $-9.0$, but this doesn't seem to be the case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDlYDWHdqQta"
   },
   "source": [
    "This completes this `pandas` introduction. Now lets move on to the actual decision tree assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "U_PELRy4qQtb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1 shape: (303, 14)\n",
      "   #3 age  #4 sex  #9 cp  #10 trestbps  #12 chol  #16 fbs  #19 restecg  \\\n",
      "0    63.0     1.0    1.0         145.0     233.0      1.0          2.0   \n",
      "1    67.0     1.0    4.0         160.0     286.0      0.0          2.0   \n",
      "2    67.0     1.0    4.0         120.0     229.0      0.0          2.0   \n",
      "3    37.0     1.0    3.0         130.0     250.0      0.0          0.0   \n",
      "4    41.0     0.0    2.0         130.0     204.0      0.0          2.0   \n",
      "\n",
      "   #32 thalachh  #38 exng  #40 oldpeak  #41 slope #44 ca #51 thal  #58 num  \n",
      "0         150.0       0.0          2.3        3.0    0.0      6.0        0  \n",
      "1         108.0       1.0          1.5        2.0    3.0      3.0        2  \n",
      "2         129.0       1.0          2.6        2.0    2.0      7.0        1  \n",
      "3         187.0       0.0          3.5        3.0    0.0      3.0        0  \n",
      "4         172.0       0.0          1.4        1.0    0.0      3.0        0  \n",
      "\n",
      "\n",
      "Dataset 2 shape: (294, 14)\n",
      "   #3 age  #4 sex  #9 cp #10 trestbps #12 chol #16 fbs #19 restecg  \\\n",
      "0      28       1      2          130      132       0           2   \n",
      "1      29       1      2          120      243       0           0   \n",
      "2      29       1      2          140        ?       0           0   \n",
      "3      30       0      1          170      237       0           1   \n",
      "4      31       0      2          100      219       0           1   \n",
      "\n",
      "  #32 thalachh #38 exng  #40 oldpeak #41 slope #44 ca #51 thal  #58 num  \n",
      "0          185        0          0.0         ?      ?        ?        0  \n",
      "1          160        0          0.0         ?      ?        ?        0  \n",
      "2          170        0          0.0         ?      ?        ?        0  \n",
      "3          170        0          0.0         ?      ?        6        0  \n",
      "4          150        0          0.0         ?      ?        ?        0  \n",
      "\n",
      "\n",
      "Dataset 3 shape: (123, 14)\n",
      "   #3 age  #4 sex  #9 cp #10 trestbps  #12 chol #16 fbs #19 restecg  \\\n",
      "0      32       1      1           95         0       ?           0   \n",
      "1      34       1      4          115         0       ?           ?   \n",
      "2      35       1      4            ?         0       ?           0   \n",
      "3      36       1      4          110         0       ?           0   \n",
      "4      38       0      4          105         0       ?           0   \n",
      "\n",
      "  #32 thalachh #38 exng #40 oldpeak #41 slope #44 ca #51 thal  #58 num  \n",
      "0          127        0          .7         1      ?        ?        1  \n",
      "1          154        0          .2         1      ?        ?        1  \n",
      "2          130        1           ?         ?      ?        7        3  \n",
      "3          125        1           1         2      ?        6        1  \n",
      "4          166        0         2.8         1      ?        ?        2  \n",
      "\n",
      "\n",
      "Dataset 4 shape: (200, 14)\n",
      "   #3 age  #4 sex  #9 cp #10 trestbps #12 chol #16 fbs  #19 restecg  \\\n",
      "0      63       1      4          140      260       0            1   \n",
      "1      44       1      4          130      209       0            1   \n",
      "2      60       1      4          132      218       0            1   \n",
      "3      55       1      4          142      228       0            1   \n",
      "4      66       1      3          110      213       1            2   \n",
      "\n",
      "  #32 thalachh #38 exng #40 oldpeak #41 slope #44 ca #51 thal  #58 num  \n",
      "0          112        1           3         2      ?        ?        2  \n",
      "1          127        0           0         ?      ?        ?        0  \n",
      "2          140        1         1.5         3      ?        ?        2  \n",
      "3          149        1         2.5         1      ?        ?        1  \n",
      "4           99        1         1.3         2      ?        ?        0  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "file_names = ['processed.cleveland.data', \n",
    "              'processed.hungarian.data', \n",
    "              'processed.switzerland.data', \n",
    "              'processed.va.data']\n",
    "\n",
    "column_names = [\n",
    "    '#3 age', '#4 sex', '#9 cp', '#10 trestbps', '#12 chol', \n",
    "    '#16 fbs', '#19 restecg', '#32 thalachh', '#38 exng', \n",
    "    '#40 oldpeak', '#41 slope', '#44 ca', '#51 thal', '#58 num'\n",
    "]\n",
    "\n",
    "data = [\n",
    "    pd.read_csv(f, header=None, names=column_names) \n",
    "    for f in file_names\n",
    "]\n",
    "\n",
    "for i, df in enumerate(data, 1):\n",
    "    print(f\"Dataset {i} shape:\", df.shape)\n",
    "    print(df.head())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CbwczDdRqQtc"
   },
   "source": [
    "Lets inspect the scope of this problem by writing the `count_missing` function, which should count the number of missing elements for each feature / column of a data set. For this you may assume that all missing elements are represented with the same symbol. The function should thus return a *Series* of missing counts, one count for each column in the data set. We have provided you with the code to print some statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "KNqgA1mPqQtc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: processed.cleveland.data\n",
      "Rows: 303\n",
      "Missing per column: \n",
      "   #3 age  #4 sex  #5 cp  #6 trestbps  #7 chol  #8 fbs  #9 restecg  \\\n",
      "0       0       0      0            0        0       0           0   \n",
      "\n",
      "   #10 thalachh  #11 exng  #12 oldpeak  #13 slp  #14 caa  #15 thall  \\\n",
      "0             0         0            0        0        4          2   \n",
      "\n",
      "   #16 output  \n",
      "0           0  \n",
      "\n",
      "Filename: processed.hungarian.data\n",
      "Rows: 294\n",
      "Missing per column: \n",
      "   #3 age  #4 sex  #5 cp  #6 trestbps  #7 chol  #8 fbs  #9 restecg  \\\n",
      "0       0       0      0            1       23       8           1   \n",
      "\n",
      "   #10 thalachh  #11 exng  #12 oldpeak  #13 slp  #14 caa  #15 thall  \\\n",
      "0             1         1            0      190      291        266   \n",
      "\n",
      "   #16 output  \n",
      "0           0  \n",
      "\n",
      "Filename: processed.switzerland.data\n",
      "Rows: 123\n",
      "Missing per column: \n",
      "   #3 age  #4 sex  #5 cp  #6 trestbps  #7 chol  #8 fbs  #9 restecg  \\\n",
      "0       0       0      0            2        0      75           1   \n",
      "\n",
      "   #10 thalachh  #11 exng  #12 oldpeak  #13 slp  #14 caa  #15 thall  \\\n",
      "0             1         1            6       17      118         52   \n",
      "\n",
      "   #16 output  \n",
      "0           0  \n",
      "\n",
      "Filename: processed.va.data\n",
      "Rows: 200\n",
      "Missing per column: \n",
      "   #3 age  #4 sex  #5 cp  #6 trestbps  #7 chol  #8 fbs  #9 restecg  \\\n",
      "0       0       0      0           56        7       7           0   \n",
      "\n",
      "   #10 thalachh  #11 exng  #12 oldpeak  #13 slp  #14 caa  #15 thall  \\\n",
      "0            53        53           56      102      198        166   \n",
      "\n",
      "   #16 output  \n",
      "0           0  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def count_missing(df):\n",
    "    # Detect missing values across the DataFrame\n",
    "    # Look for common missing value indicators\n",
    "    missing_mask = (df == '?') | (df == -9)\n",
    "    return missing_mask.sum()\n",
    "\n",
    "\n",
    "for i in range(len(data)):\n",
    "    print(f'Filename: {file_names[i]}')\n",
    "    print(f'Rows: {len(data[i])}')\n",
    "\n",
    "    # The following code makes sure that we print horizontally and not vertically\n",
    "    print(f'Missing per column: \\n{count_missing(data[i]).to_frame().T}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jwi68rajqQtc"
   },
   "source": [
    "### Cleaning the data [2 pts]\n",
    "\n",
    "Looking at the results from the previous step, it seems like the sets from some hospitals are more complete than others. There are different approaches you might take to solve this, like replacing the missing values with the average value for that variable, or handling missing values within the algorithm in a seperate way. For now we will take the simplest approach, discarding any rows that contain missing values. This way we only use the complete patient records from each data set.\n",
    "\n",
    "We will concatenate all four DataFrames into a single DataFrame called `df`. Drop any rows containing a missing value. You can use pandas `pd.to_numeric` with keyword `error='coerce'` to easily convert entries to floats. When trying to convert the missing values, the DataFrame will transform these into `NaN` (Not a Number) instead of a float. After you remove the rows with `NaN`, you should end up with about 300 patient records, most of which are from the Cleveland hospital. Each patient has 14 variables, one of which (the 14th variable) is the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BELIdVyTqQtc"
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.concat(data, ignore_index=True)\n",
    "\n",
    "df = df.replace('?', np.nan)\n",
    "\n",
    "for col in df.columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "assert len(df) == 299\n",
    "assert len(df.columns) == 14\n",
    "target = df.columns[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1b3wy8_4qQtc"
   },
   "source": [
    "Lets take a look at how often each label in the target variable actually occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "-bqETeFjqQtd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 160, 1: 56, 2: 35, 3: 35, 4: 13})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Codegrade Tag18\n",
    "from collections import Counter\n",
    "\n",
    "Counter(df[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsTsDW47qQtd"
   },
   "source": [
    "We can see that the number of each of the labels of our target variable differs greatly. While there are 160 labels with the value $0$, we only have 13 labels with value $4$. In virtually all classification tasks, training data that contains different numbers of representatives from each class might result in a classifier that is biased towards the most common class. When applied to a test set that is similarly imbalanced, this classifier yields an optimistic accuracy estimate. In an extreme case, the classifier might assign every single test case to the majority class, thereby trivially achieving an accuracy equal to the proportion of test cases belonging to the majority class!\n",
    "\n",
    "In the description file ([heart-disease.names](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/heart-disease.names)) it says *Value 0: < 50% diameter narrowing* and *Value 1: > 50% diameter narrowing*, but it does not explain all the other values. It might be logical to assume that these are different degrees of narrowing, where $0$ would mean no disease and higher values would mean different levels of disease present. Because the distribution of the different values is so skewed however, for now we will just focus on classifying the difference between a value of $0$  and any of the higher values.\n",
    "\n",
    "Change the target column to contain a boolean value that is `True` if there is more than $50\\%$ narrowing and `False` otherwise. Then print the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "4fodCuzTqQtd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({False: 208, True: 91})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['narrow_disease'] = df[target].apply(lambda x: x > 0 and x <= 2)\n",
    "\n",
    "Counter(df['narrow_disease'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnNumlMdqQtd"
   },
   "source": [
    "# Categorical Decision Trees\n",
    "\n",
    "If you have not yet read sections **3.1-3.4** of \"Machine Learning\" by Tom Mitchell, do so now. The entire chapter 3 on Decision Trees is linked on the Canvas homepage.\n",
    "\n",
    "The main idea of decision trees is to find the feature that contains the most \"information\" and then split/group the dataset along the rows of this feature that have the same value. This process of finding the \"most informative\" feature and then splitting is repeated until we arrive at a stopping criterium.\n",
    "\n",
    "We will implement this process over several steps:\n",
    "- Generate splits from a given dataset\n",
    "- Calculate the Shannon entropy; a measure of the amount of information in a given dataset\n",
    "- Calculate the Information Gain for a given split\n",
    "- Combine these metrics into an algorithm that creates the Decision Tree\n",
    "\n",
    "## Splits [3 pts]\n",
    "\n",
    "Before we can determine which of our features produce the most descriptive split, we must create each split. **A split is a grouping of rows in a dataset by each of the unique values in _one_ of the columns.**\n",
    "\n",
    "First, we need to determine what unique values are present in a column. Implement a function named `unique_values` that takes a dataframe `df` and a column name `m` and returns a list of the unique values in the given column. Test the function by entering a couple of the column names and see if the outcome is as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "4Ozk8xSHqQte"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'age'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/indexes/base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'age'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34munique_values\u001b[39m(df, m):\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m### YOUR SOLUTION HERE\u001b[39;00m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(df[m].unique())\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43munique_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mage\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(unique_values(df, \u001b[33m'\u001b[39m\u001b[33msex\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(unique_values(df, \u001b[33m'\u001b[39m\u001b[33mchol\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36munique_values\u001b[39m\u001b[34m(df, m)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34munique_values\u001b[39m(df, m):\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m### YOUR SOLUTION HERE\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mm\u001b[49m\u001b[43m]\u001b[49m.unique())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'age'"
     ]
    }
   ],
   "source": [
    "# Codegrade Tag20\n",
    "def unique_values(df, m):\n",
    "    ### YOUR SOLUTION HERE\n",
    "    return sorted(df[m].unique())\n",
    "\n",
    "print(unique_values(df, 'age'))\n",
    "print(unique_values(df, 'sex'))\n",
    "print(unique_values(df, 'chol'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_F4kDC8qQte"
   },
   "source": [
    "Now implement a function named `create_split` that takes a dataframe `df` and a column name `m` and returns a dictionary. This dictionary has the *unique column values* as its keys, and the split dataframes as its values. Remember that you just wrote a function for getting the unique keys! Each of the dataframes consist of only data rows that match with the same unique value. Simply put, the data frame rows are \"grouped\" by each of the different unique values from that column.\n",
    "\n",
    "*Note:* When adding each of the frames in the dictionary, remove the column that was used to create the split. We have already split it into its different unique values, so the information in that column is now redundant and further splits on this column would not be possible.\n",
    "\n",
    "You should end up with a dictionary that has a set of rows for each unique value in the given column. Test the function by entering a column name, and see if the outcome is as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_-pNPh1qQte"
   },
   "outputs": [],
   "source": [
    "# Codegrade Tag21\n",
    "def create_split(df, m):\n",
    "    ### YOUR SOLUTION HERE\n",
    "\n",
    "    return ???\n",
    "\n",
    "print(create_split(df, 'age')[55])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBTfIJWhqQte"
   },
   "source": [
    "## Entropy [4 pts]\n",
    "There are quite a few different definitions of what entropy is; all of them relate to the notion of chaos / order in a system, but the exact definition strongly depends on the context in which the term is used. Most commonly the term refers to thermodynamic entropy, where it describes the number of possible configurations a thermodynamic system can have in a specific state. This is related to the idea of a universal entropy, as used in Asimov's classic short story The Last Question. For decision trees we need the information theoretic entropy, or Shannon entropy, which says something about the amount of information contained in a distribution of data. The more ordered or one-sided the distribution is, the less bits we would need on average to express the exact distribution.\n",
    "\n",
    "We will use this measure of entropy to compare the results of decision tree splits to see which is the \"most informative\". For the heart disease problem there are now only 2 class labels we are considering, `True` if *vascular narrowing >= 50% diameter* and `False` otherwise. For a 2 class problem, the entropy is defined as:\n",
    "\n",
    "(3.1) $$\\phi(p) = −p\\ log_2(p) − (1 − p)\\ log_2(1 − p)$$\n",
    "\n",
    "where $p$ is the ratio between between the labels for class 1 and class 2.\n",
    "\n",
    "First, write a `ratio` function to compute $p$. The function should, given a list of boolean values as class labels, return the ratio of `True` labels in the list, e.g. $1.0$ would indicate the list only contained `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twLGWwesqQtf"
   },
   "outputs": [],
   "source": [
    "# Codegrade Tag22\n",
    "def ratio(labels):\n",
    "    ### YOUR SOLUTION HERE\n",
    "\n",
    "    return ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8qdhKwqqQtf"
   },
   "source": [
    "Now lets assume that $p=0$; meaning there are no labels that equal `True`. The computation of $-0\\ log_2(0)$ would then (correctly) result in a math error, however this could also just be defined as having the value $0$ (as it is multiplied by $0$).\n",
    "\n",
    "Write the function `entropy_sub` to compute the value of the log product ($-p\\ log_2(p)$), making sure to return $0$ in the case that $p = 0$. Combine `ratio` and `entropy_sub` to compute the `entropy` of a list of boolean class labels (equation 3.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGZW8HxXqQtf"
   },
   "outputs": [],
   "source": [
    "# Codegrade Tag23\n",
    "import math\n",
    "\n",
    "def entropy_sub(p):\n",
    "    ### YOUR SOLUTION HERE\n",
    "\n",
    "    return ???\n",
    "\n",
    "def entropy(labels):\n",
    "    ### YOUR SOLUTION HERE\n",
    "\n",
    "    return ???\n",
    "\n",
    "print(entropy_sub(.6) + entropy_sub(.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbDz6BtsqQtf"
   },
   "source": [
    "Write a `plot_entropy` function to check whether the entropy function works correctly. This function should create many different lists of boolean labels of length N and compute ratio and entropy for each of these lists. Note that for a list of booleans of length 𝑁, there are only 𝑁+1 different possible ratios of labels you need to create. The x-axis of your plot should show the ratios and the y-axis their resulting entropies, which should produce a graph like Figure 3.2 in Mitchell (Remember that the log is in base 2). Show this plot at the end of your code and make sure it looks correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SrOh8867qQtf"
   },
   "outputs": [],
   "source": [
    "# Codegrade Tag24\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_entropy(N):\n",
    "    ### YOUR SOLUTION HERE\n",
    "\n",
    "    return ratios, entropies\n",
    "\n",
    "\n",
    "r, e = plot_entropy(N=10)\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "\n",
    "plt.plot(r, e, 'b-', linewidth=7)\n",
    "\n",
    "plt.xlabel(\"Ratio\", fontsize=18)\n",
    "plt.ylabel(\"Entropy\", fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hynlpHQgqQtg"
   },
   "source": [
    "## Information gain [3 pts]\n",
    "There are several metrics we can use to asses how good a split is in a decision tree. For this implemenation we will use the *Information Gain*, which is defined as the entropy of the original distribution $\\phi(p)$, minus the entropy of the split distribution $I_m$ resulting from the split on variable $m$.\n",
    "\n",
    "(3.4a) $$IG_m = \\phi(p) - I_m$$\n",
    "\n",
    "Information Gain measures how much the entropy changes from making a specific split, i.e. the gain in predictablity of the data as a result of making a distribution based on a specific variable. When a set of target labels is split on a variable $m$, two or more new lists are created, each with their own entropy. Combining the resulting entropies from a split into $s$ new sets is a simple weighted sum:\n",
    "\n",
    "(3.4b) $$I_m = \\sum_{j=1}^s \\frac{N_j}{N} \\phi(p_j)$$\n",
    "\n",
    "where $N_j$ is the size of the $j^{th}$ split distribution, $p_j$ is the ratio of the target labels for that same $j^{th}$ distribution and $N$ is the size of the distribution before the split.\n",
    "\n",
    "Write the function `split_entropy` to compute $I_m$ for some list of target labels and a given `N`. The `split_labels` argument is a list containing $s$ different lists, each containing the target labels for one part of the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "byg4NrzyqQtg"
   },
   "outputs": [],
   "source": [
    "# Codegrade Tag25\n",
    "def split_entropy(split_labels, N):\n",
    "    ### YOUR SOLUTION HERE\n",
    "\n",
    "    return ???\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-0EUSnLqQtg"
   },
   "source": [
    "Currently, our function `create_split` returns a dictionary where the keys are the unique entries present in the column chosen and the values are dataframes that contain the rows that have those unique entries. However, the function `split_entropy` expects a list wherein each element is a list of target labels.\n",
    "\n",
    "Write the function `get_split_labels` that accepts a `split` dictionary and `target` (the name of the column that contains the labels) and returns the `split_labels`. This returned list of lists should be of the same format as before, so it can be used directly as input for the `split_entropy` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LmxBysZlqQtg"
   },
   "outputs": [],
   "source": [
    "# Codegrade Tag26\n",
    "def get_split_labels(split, target):\n",
    "    ### YOUR SOLUTION HERE\n",
    "\n",
    "    return ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odCwKKiFqQtg"
   },
   "source": [
    "Write the `information_gain` function using your earlier functions `entropy` and `split_entropy`. Assume `split_labels` is a list containing $s$ different lists, each containing the target labels from one of each  of the splits. Remember that one split exists of all of the grouped unique values in a column of our data, thus, the total number of target labels given to this function is $N$ and the combination of all labels is $p$. You can use `pd.concat` to \"glue\" the different Series-objects together to compute $p$ directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XuGD8AsHqQtg"
   },
   "outputs": [],
   "source": [
    "# Codegrade Tag27\n",
    "def information_gain(split_labels):\n",
    "    ### YOUR SOLUTION HERE\n",
    "\n",
    "    return ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_JX4zqYqQtg"
   },
   "source": [
    "Below, we have created two assertions that check whether your functions were implemented correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ve0ZMjTqQth"
   },
   "outputs": [],
   "source": [
    "# Codegrade Tag28\n",
    "# We create a copy, because we remove the target in create_split, while it is needed for split_labels\n",
    "test_df = df.copy()\n",
    "target_copy = target + '_copy'\n",
    "test_df[target_copy] = test_df[target]\n",
    "\n",
    "split = create_split(test_df, target_copy)\n",
    "split_labels = get_split_labels(split, target)\n",
    "\n",
    "assert entropy(test_df[target].tolist()) == information_gain(split_labels)\n",
    "assert split_entropy(split_labels, len(test_df)) == 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6OOzM3LqQth"
   },
   "source": [
    "## ID3\n",
    "\n",
    "So far, we have determined the total purity of the dataset (entropy), and created the functions that enable us to determine what column of our data is the most informative (Information Gain). That leads us to the introduction of the *ID3* algorithm introduced by Ross Quinlan in 1986. This algorithm can be used to create a categorical decision tree through a process of greedy iterations. Reread table 3.1 in Mitchell's 'Machine Learning', containing the pseudo-code for the algorithm.\n",
    "\n",
    "Our version of the *ID3* algorithm can be described as follows:\n",
    "\n",
    "1. *If the current dataset is pure (all rows have the same label), return the pure label.* Use the `unique_value` function for this.\n",
    "2. *If there are no more possible columns to split; return the most common label.*\n",
    "3. *Calculate the Information Gain of each possible split in the dataset.* First create splits for each of the columns in the dataset (except for the target column), then extract the label lists for each of these splits and use these to calculate the Information Gain of that split.\n",
    "4. *Pick the split with the largest Information Gain.*\n",
    "5. *Create a Node for this new/current branch of the tree.*\n",
    "6. *For each of the sub-datasets in the split, create a sub-tree (recursively call `ID3` with the sub-dataset) and use the sub-tree result as the branches of the tree created in step 5.*\n",
    "7. *Return the complete tree.*\n",
    "\n",
    "We have also provided you with a [namedtuple](https://docs.python.org/3/library/collections.html#collections.namedtuple) `Node` that can hold the name of the selected split column; `column_name`, the most common label in this node; `mode`, and a dictionary mapping to results of the split; `branches`, wherein the key is each of the 'unique values' possible, and the values are the result of the algorithm on the data available for that unique value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rfNGIyf2qQth"
   },
   "outputs": [],
   "source": [
    "# Codegrade Tag29\n",
    "from collections import namedtuple\n",
    "\n",
    "Node = namedtuple('Node', ['column_name',  'mode', 'branches'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mH8pj2c2qQth"
   },
   "source": [
    "### Example\n",
    "\n",
    "Lets take a look at an example before we start writing out the complete algorithm. Say we have the following data set and want to classify on the target column *Vegetable*:\n",
    "\n",
    "|&nbsp;         | Color  | Pits  | Vegetable |\n",
    "|---------------|--------|-------|-----------|\n",
    "| **Broccoli**  | Green  | False | True      |\n",
    "| **Pear**      | Green  | True  | False     |\n",
    "| **Strawberry**| Red    | False | False     |\n",
    "| **Zucchini**  | Green  | False | True      |\n",
    "| **Orange**    | Orange | True  | False     |\n",
    "\n",
    "\n",
    "We won't bother with computing the exact Information Gain here, but looking at the 2 possible columns and the distribution of the target column *Vegetable*, you might conclude that *Color* would be a good variable to split on.\n",
    "\n",
    "We can use the [*namedtuple*](https://docs.python.org/3/library/collections.html#collections.namedtuple) to create a `Node` in our Decision Tree and have it store all the relevant information at that level of the tree. The `column_name` to split on has already been chosen to be *Color* and the `mode` is the most common label of the target column in this node, so this would be *False*, as there are currently more fruits than vegetables in our data set. The `branches` would start out by just being the result of the `create_split` function, meaning there would be 3 new data frames, one for each *Color*. The whole `Node` would then look something like\n",
    "\n",
    "    Node(column_name='Color', mode=False, branches={\n",
    "    \n",
    "            'Green':\n",
    "\n",
    "|&nbsp;         | Pits  | Vegetable |\n",
    "|---------------|-------|-----------|\n",
    "| **Broccoli**  | False | True      |\n",
    "| **Pear**      | True  | False     |\n",
    "| **Zucchini**  | False | True      |\n",
    "\n",
    "            'Orange':\n",
    "            \n",
    "|&nbsp;         | Pits  | Vegetable |\n",
    "|---------------|-------|-----------|\n",
    "| **Orange**    | True  | False     |\n",
    "\n",
    "            'Red':\n",
    "\n",
    "|&nbsp;         | Pits  | Vegetable |\n",
    "|---------------|-------|-----------|\n",
    "| **Strawberry**| False | False     |\n",
    "            \n",
    "            })\n",
    "    \n",
    "However, this is of course only 1 split of our data. The crucial step in the *ID3* algorithm is step **6**; recursively create the sub-trees. So we want to split each of these data sets again and if those produce new branches, we split those further *again*, etc. This is what creates the actual tree structure, the recursive splitting into nodes, each with new branches.\n",
    "\n",
    "As you hopefully remember from your first year, all recursions need base cases and that is what steps **1 and 2** are for. The *leaf nodes* are reached when we've already perfectly split the data or no more splits are possible, and in those cases we just return the predicted label. The whole purpose of the tree structure is to predict a label for the target column. So when we reach the end of some path in the Decision Tree, all we really care about is what the predicted target label would be at that leaf in the tree, and so that is the only information we will need to store there.\n",
    "\n",
    "Combining these steps, *any* branch in a `Node` should always point to another `Node` or the predicted label for that point in the tree. This means the example structure above, containing 3 data frames, would never actually be returned by the algorithm, as the recursive step to split each frame down to *leaf nodes* is still missing. The result of that would look something like\n",
    "\n",
    "    Node(column='Color', mode=False, branches={\n",
    "    \n",
    "        'Green': Node(column='Pits', mode=True, branches={\n",
    "                \n",
    "                    False: True,\n",
    "\n",
    "                    True: False}),\n",
    "\n",
    "        'Orange': False,\n",
    "        \n",
    "        'Red': False})\n",
    "\n",
    "Note that each terminal value (i.e. leaf node) is now either *True* or *False*, indicating the predicted label for the *Vegetable* column at that point in the tree. If we try and interpret this tree, there is only 1 case where it would predict that something is indeed a vegetable; when it is both *Green* and has *No Pits*. In all other cases this tree would predict it to be a fruit. Verify both the constructed tree and the resulting interpreted rule before moving on to implementation.\n",
    "\n",
    "### Implementation [5 pts]\n",
    "\n",
    "Now that we know what the tree structure will look like, lets build the *ID3* algorithm. So far, we have already included step 5 and 7 for you. Step 5 expects the variables `column_name` and `mode` to already be defined by you, in order to create the `Node`, while the branches start out empty.\n",
    "\n",
    "Implement the other steps and see if your code produces that same tree on `test_df`, which contains the vegetable example. Note that the *white space* was added to make the example more readable, so your actual result should look like:\n",
    "\n",
    "    Node(column_name='Color', mode=False, branches={'Green': Node(column_name='Pits', mode=True, branches={False: True, True: False}), 'Orange': False, 'Red': False})\n",
    "\n",
    "For your convenience we will repeat the steps of the _ID3_ algorithm (this is the same text as you can find above):\n",
    "1. *If the current dataset is pure (all rows have the same label), return the pure label.* Use the `unique_value` function for this.\n",
    "2. *If there are no more possible columns to split; return the most common label.* HINT: You can use pandas [`mode()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mode.html).\n",
    "3. *Calculate the Information Gain of each possible split in the dataset.* First create splits for each of the columns in the dataset (except for the target column), then extract the label lists for each of these splits and use these to calculate the Information Gain of that split.\n",
    "4. *Pick the split with the largest Information Gain.*\n",
    "5. *Create a Node for this new/current branch of the tree.*\n",
    "6. *For each of the sub-datasets in the split, create a sub-tree (recursively call `ID3` with the sub-dataset) and use the sub-tree result as the branches of the tree created in step 5.*\n",
    "7. *Return the complete tree.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9gj6cJdxqQth"
   },
   "outputs": [],
   "source": [
    "# Codegrade Tag30\n",
    "\n",
    "test_df = pd.DataFrame([['Green', False, True], ['Green', True, False], ['Red', False, False],\n",
    "                        ['Green', False, True], ['Orange', True, False]],\n",
    "                       index=['Broccoli', 'Pear', 'Strawberry', 'Zucchini', 'Orange'],\n",
    "                       columns=['Color', 'Pits', 'Vegetable'])\n",
    "\n",
    "test_target = 'Vegetable'\n",
    "\n",
    "def ID3(data, target):\n",
    "    # step 1\n",
    "    ### YOUR SOLUTION HERE\n",
    "\n",
    "\n",
    "    # step 2\n",
    "    ### YOUR SOLUTION HERE\n",
    "    # check if there's just the target left\n",
    "\n",
    "\n",
    "    # step 3\n",
    "    ### YOUR SOLUTION HERE\n",
    "\n",
    "\n",
    "    # step 4\n",
    "    ### YOUR SOLUTION HERE\n",
    "\n",
    "\n",
    "    # step 5\n",
    "    branches = {}\n",
    "    tree = Node(column_names[max_info_gain_idx], label_mode, branches)\n",
    "\n",
    "    # step 6\n",
    "    ### YOUR SOLUTION HERE\n",
    "\n",
    "\n",
    "    # step 7\n",
    "    return tree\n",
    "\n",
    "tree = ID3(test_df, test_target)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xh2DEp28qQti"
   },
   "source": [
    "\n",
    "Now that we have built a tree, it is important to realise that this algorithm *does not* search the entire space of possible trees for the best possible decision tree, but instead opts for the step at each node that will make the most progress at that moment. The tree that results from these locally optimal choices might not be globally optimal (although that happens to be the case for this small toy example).\n",
    "\n",
    "## Classify [3 pts]\n",
    "\n",
    "With this complete tree classifying a new data-entry is pretty easy! Implement the `classify` function, which accepts a tree (a variable containing a `Node` representing the root of the tree), and a *single row* of a DataFrame, i.e. a Series. The function should return the predicted target label for that one row of data.\n",
    "\n",
    "Continuing the example from before; we have a new element from that same data set, but we don't yet know if it would be considered a vegetable, so we want to try and use our decision tree to predict if it is, e.g.\n",
    "\n",
    "|&nbsp;         | Color  | Pits  |\n",
    "|---------------|--------|-------|\n",
    "| **Tomato**    | Red    | False |\n",
    "\n",
    "The function should move through the relevant branches down the tree based on the `column_name` at each node and the corresponding value of the row we are trying to predict. There are 3 options at each branch:\n",
    "\n",
    "1. There is another Node attached to this branch, meaning we should continue further down the tree.\n",
    "2. This branch leads to a *leaf node*, meaning we now have a predicted label for the row we can return.\n",
    "3. The value from the column in our row actually does not have a corresponding branch in this node. In that case the function should return the most common label in that node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aj9Qh29fqQti"
   },
   "outputs": [],
   "source": [
    "# Codegrade Tag31\n",
    "def classify(tree, row):\n",
    "    ### YOUR SOLUTION HERE\n",
    "\n",
    "    return ???\n",
    "\n",
    "\n",
    "test_tomato = pd.Series(['Red', False], index=['Color', 'Pits'])\n",
    "assert not classify(tree, test_tomato), \"Tomato is a fruit!\"\n",
    "\n",
    "test_grape = pd.Series(['Blue', True], index=['Color', 'Pits'])\n",
    "assert not classify(tree, test_grape), \"Grape is a fruit!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76EGbLevqQti"
   },
   "source": [
    "Note that the 3rd option can occur as it is possible the categorical value that is being classified was not present in the training set at that point in the tree. This means that even though there are more branches further down the tree, none of them match the row we are trying to classify. Therefore, the best prediction we can possible give for that row, is simply the most common target label that was present in the training set at that node in the tree.\n",
    "\n",
    "It could be the case that we have never seen this categorical value in our training set before at all, as with the grape example above. However, it is also possible that a specific categorical value *did* occur in our training set, but still some nodes *do not* have a branch for that value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwCptTFhqQti"
   },
   "source": [
    "## Validation [3 pts]\n",
    "\n",
    "With all these elements complete, we can start applying the *ID3* algorithm on the heart disease data set and see how well it can predict the *Vascular narrowing* based on the other features describing each patient.\n",
    "\n",
    "Before we see the accuracy of our Decision Tree, we need to create a validation split. Split the DataFrame into `train` and `test` using a ratio of $0.7$ with [sklearn's `train_test_split` function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PL1LSi-VqQti"
   },
   "outputs": [],
   "source": [
    "# Codegrade Tag32\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "??? = train_test_split(???, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iET-BjN2qQtj"
   },
   "source": [
    "We will also need to split the train and test DataFrames into categorical and numerical features, using `cat_num_split`. This could be done manually by taking a good look at the description file ([heart-disease.names](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/heart-disease.names)), but it can also be done by counting the amount of unique values present in a column. As a rule of thumb, it is probably unlikely there are more than 10 different categorical values for any one column, while it is seems very likely that there are more than 10 different numerical values for a column present in the data.\n",
    "\n",
    "Use the `unique_values` function from earlier and select the variables that have no more than the `threshold` argument number of unique values as categorical variables and all others as numerical variables. The categorical variables are all whole numbers, so the resulting 2d-array should be of type `int` and the numeric variables array should of type `float`, for which you can use pandas `astype` method. Remember that both dataframes should have the target column.\n",
    "\n",
    "Apply this function to separate out the categorical and numerical features for both the training and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V04mnuUUqQtj"
   },
   "outputs": [],
   "source": [
    "# Codegrade Tag33\n",
    "def cat_num_split(data, target, threshold=10):\n",
    "    ### YOUR SOLUTION HERE\n",
    "\n",
    "    return ???\n",
    "\n",
    "train_cat, train_num = cat_num_split(df_train, target)\n",
    "test_cat, test_num = cat_num_split(df_test, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OI6w1NBqQtj"
   },
   "source": [
    "Finally, write the function `validate` that takes a decision tree, a dataframe of testing sample, and the target column name. It should `classify` all rows in the dataframe using the decision tree and return the percentage of elements that was classified correctly.\n",
    "\n",
    "\n",
    "Create a decision tree fitted to the categorical training data created above. Validate the results by computing both the train and test accuracy using the categorical features of the heart disease data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b1oYJHtRqQtj"
   },
   "outputs": [],
   "source": [
    "# Codegrade Tag34\n",
    "def validate(tree, data, target):\n",
    "    ### YOUR SOLUTION HERE\n",
    "\n",
    "    return ???\n",
    "\n",
    "\n",
    "cat_tree = ID3(train_cat, target)\n",
    "print(\"Train Accuracy: %.1f%%\" %(validate(cat_tree, train_cat, target) * 100))\n",
    "print(\"Test Accuracy: %.1f%%\" %(validate(cat_tree, test_cat, target) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P62j1IPVqQtj"
   },
   "source": [
    "# Numerical Decision Tree [2 pts]\n",
    "\n",
    "Now that our Categorical Decision Tree is done, we will take a look at a Numerical Decision tree. Start by reading section **3.7.2** from Mitchell. As stated there, it is possible to extend the Categorical Decision Tree to also include numerical boundaries. We could then use this to even select the best Information Gain from *both* the categorical *and* the numerical splits at each node.\n",
    "\n",
    "However, for now we will focus a model that can *only* make numerical splits, as that is what the `scikit-learn` library provides straight out of the box: The [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) is a class that can build a decision tree from numerical data, and so we won't have to implement this tree from scratch.\n",
    "\n",
    "The most important difference between categorical and numerical decision trees is the way the data is split. In categorical data, it is easy to split the tree into the $N$ categories that are present in the data. This is impossible to do in numerical data, as there are infinitely many categories. Categorical Decision trees therefore use binary splits or a so called split boundary; where with each split we create one branch with values smaller than the boundary, and one branch with values greater than or equal to the boundary.\n",
    "\n",
    "This boundary for a column can be determined by trying every possible split boundary available for the set of values. This is done by first sorting the samples and then trying every split half way between two neighbouring values that have different labels. This means there can be as many splits as there are samples, and as such this method is computationally very expensive. An alternative, simpler method that is often used, is trying some amount of random splits for a column and picking the best random split among them.\n",
    "\n",
    "Note that repeated binary splits on the same numerical variable can be used to create many different \"decision regions\" for the same variable. For example, consider a label where you want a variable to be above or equal to 3.4, but below 4.8. Here you would need two splits; a first split with 3.4 as the boundary and then in the \"greater equal branch\" of that split, another split boundary of 4.8 on that same variable. This means that while the feature that was split on could be ignored in further splits in the case of a Categorical Decision tree, this is not the case for Numerical Decision Trees, and repeated splits on the same variable can actually greatly improve accuracy.\n",
    "\n",
    "Implement a Numerical Tree Classifier using a `sklearn.tree.DecisionTreeClassifier` and its `fit` and `predict` functions.  Train it using the numerical part of the dataset you split out earlier, and print the training and test accuracies (this can easily be done through the `metrics.accuracy_score` method from the `metrics` module). Note that the `fit` function of the classifier requires you to separate the target column from the dataframe and provide the training set as `X` and `y`; you can use `.drop(target, axis=1)` for this, as it returns a new dataframe, but without the target class column.  The targets should be passed into the sci-kit functions as integers; you can use `.astype('int')` to cast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTGX7JlTqQtk"
   },
   "outputs": [],
   "source": [
    "# Codegrade Tag35\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "\n",
    "print(\"Train Accuracy: %.1f%%\" %(???))\n",
    "print(\"Test Accuracy: %.1f%%\" %(???))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VC2t6q-7qQtk"
   },
   "source": [
    "The scikit-learn Decision Trees implemenation is specifically intended for numerical features. This might not be immideately obvious, but if we read the documentation on [Decision Trees](https://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart) and see what algorithms it uses, we find:\n",
    "\n",
    "```scikit-learn uses an optimised version of the CART algorithm; however, scikit-learn implementation does not support categorical variables for now.```\n",
    "\n",
    "*Note: the linked page on Decision Trees contains more useful information and hints if you plan to use this model in any projects of your own, so it might be worth reading through.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAOxDlcmqQtk"
   },
   "source": [
    "### Plotting the Decision Tree [1 pt]\n",
    "\n",
    "Sklearn also comes with a tool that can actually plot the whole decision tree. However, plotting the complete tree would be a bit hard to read, as it wouldn't really fit in the figure. We can again use another `DecisionTreeClassifier` argument to limit the size of the tree. Here using `max_depth` makes the most sense, as we want uniformly cut the tree at certain depth for the plot.  Fit a decision tree of depth 3 (to the numerical categories) and call it `d_tree`. The code to plot the tree has already been provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E90liskyqQtk"
   },
   "outputs": [],
   "source": [
    "# Codegrade Tag36\n",
    "from sklearn import tree\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "max_depth = 3\n",
    "### YOUR SOLUTION HERE\n",
    "\n",
    "\n",
    "### YOUR SOLUTION ENDS HERE\n",
    "\n",
    "figure(figsize=(15, 10))\n",
    "tree.plot_tree(d_tree, feature_names=train_num.columns.values, class_names=['Healthy', 'Heart Disease'], impurity=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdRlT3ufqQtk"
   },
   "source": [
    "Read out the boundaries set in the decision tree. Refer back to the\n",
    "[heart-disease.names](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/heart-disease.names) if you don't know what a variable means.\n",
    "\n",
    "### Which features seem to be the best indicators for heart disease? [1 pt]\n",
    "\n",
    "**<span style=\"color:red\">YOUR ANSWER HERE</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8oKQrSyqQtk"
   },
   "source": [
    "### Depth and Held-Out Accuracy [1 pt]\n",
    "\n",
    "The depth of the decision tree relates to how expressive a decision function it can represent: the more branches, the better its ability to carve up the data.  A popular way to prevent overfitting is to limit the depth of the tree (as we did above, for visualization purposes)---making it analogous to the k (complexity parameter) in polynomial regression and k-means.  \n",
    "\n",
    "Below, make a plot of the relationship between train accuracy, test accuracy, and depth.  Make the max tree depth the x-axis and accuracy the y-axis.  Train a sci-kit learn `DecisionTreeClassifier` on the numerical features for each depth between 1 and 15 (inclusive).  Then plot the train and test accuracy so that you can see how they evolve with depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90Xh-X_vqQtk"
   },
   "outputs": [],
   "source": [
    "# Codegrade Tag37\n",
    "depth_limit = 15\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "### YOUR SOLUTION STARTS HERE\n",
    "\n",
    "\n",
    "### YOUR SOLUTION ENDS HERE\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "plt.plot([x+1 for x in range(depth_limit)], train_accs, \"r-\", lw=5, label=\"Train Accuracy\")\n",
    "plt.plot([x+1 for x in range(depth_limit)], test_accs, \"b-\", lw=5, label=\"Test Accuracy\")\n",
    "\n",
    "plt.legend(loc=2)\n",
    "plt.xlabel(\"Max Tree Depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSdqvhlvqQtl"
   },
   "source": [
    "### How does the accuracy change with depth? [1 pt]\n",
    "\n",
    "**<span style=\"color:red\">YOUR ANSWER HERE</span>**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "jdRlT3ufqQtk",
    "OSdqvhlvqQtl"
   ],
   "provenance": [
    {
     "file_id": "1K31L3YcBiUMJOgMbgSHR-9MB9deL2KQd",
     "timestamp": 1638134072852
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
