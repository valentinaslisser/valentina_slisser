{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valentinaslisser/valentina_slisser/blob/main/ExactInference_student(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x3gSyH8Ihtfw"
      },
      "id": "x3gSyH8Ihtfw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f3e55312",
      "metadata": {
        "id": "f3e55312"
      },
      "source": [
        "**Start with your name(s) and student number(s)**\n",
        "\n",
        "Team:\n",
        "\n",
        "*Valentina Slisser (14955792)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a07cbaba",
      "metadata": {
        "id": "a07cbaba"
      },
      "source": [
        "# Outline\n",
        "\n",
        "In this notebook we will develop algorithms for **exact sum-product inference** in PGMs, specifically via  the _Variable Elimination_ (VE) algorithm.\n",
        "\n",
        "VE uses the structure of the graph to (more) efficiently answer probability queries such as marginals and conditionals. There is a variant of VE that can be used for a related (but different) type of inference, the so-called max-product inference, but we will not be demonstrating max-product inference in this notebook.\n",
        "\n",
        "We will use an example of reasoning w.r.t. a medical diagnosis to build up the intuition and the methods.\n",
        "\n",
        "This is a high-level outline of the notebook, you will find exercises in most sections.\n",
        "\n",
        "1. We start with constructing the BN (DAG + CPD)\n",
        "2. We will implement and reason about naive sum-product inference for probability queries\n",
        "3. Next, we will implement and reason about the VE algorithm\n",
        "4. We will optimize VE, using graphical separation\n",
        "5. Then, we will look at different orders of eliminating variables and how this affects VE's efficiency\n",
        "6. Finally, there is a bonus exercise on ordering.\n",
        "\n",
        "**Table of Exercises**\n",
        "\n",
        "The exercises and the points they are worth are shown below:\n",
        "\n",
        "\n",
        "1. Exercise - Naive Marginalization [2]\n",
        "2. Exercise - Basic VE [2]\n",
        "3. Exercise - VE and Graphical Separation - Part I [1]\n",
        "4. Exercise - VE and Graphical Separation - Part II [2]\n",
        "5. Exercise - Order heuristics - Part I [2]\n",
        "6. Exercise - Order Heuristics - Part II [1]\n",
        "7. Exercise - Bonus [1]\n",
        "\n",
        "There are 11 points above, but the assignment grade is capped at 10. That is, `grade = min(10, \"total points acquired\")`.\n",
        "\n",
        "**Use of AI tools**\n",
        "\n",
        "In this course we expect _you_ and your team members to author your work.\n",
        "AI tools are not to be used for drafts, nor code completion, nor revisions, nor as a 'study tool', nor as a source of feedback. If you do use AI, it should not contribute to the substance of what you present as your work.  \n",
        "\n",
        "At the end of this notebook you will find a section on _Use of AI tools_. **Make sure to read and complete it**.\n",
        "By submitting a version of this notebook for assessment, you agree with our terms."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30c18c7e",
      "metadata": {
        "id": "30c18c7e"
      },
      "source": [
        "# Setting Up\n",
        "\n",
        "Take care of dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25be7325",
      "metadata": {
        "id": "25be7325"
      },
      "outputs": [],
      "source": [
        "# !pip install tabulate\n",
        "# !pip install --upgrade --force-reinstall  git+https://github.com/probabll/pgmini.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "892a302f-7769-4909-8fc1-8b99f232e8de",
      "metadata": {
        "id": "892a302f-7769-4909-8fc1-8b99f232e8de"
      },
      "outputs": [],
      "source": [
        "import pgmini\n",
        "assert pgmini.__version__ == '0.3.0', \"Don't forget to update pgmini and restart your kernel\\n!pip install --upgrade --force-reinstall  git+https://github.com/probabll/pgmini.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "276f1c0e-d14f-45d9-9065-681ff81d66cc",
      "metadata": {
        "id": "276f1c0e-d14f-45d9-9065-681ff81d66cc"
      },
      "outputs": [],
      "source": [
        "from pgmini.m1 import OutcomeSpace, DAG\n",
        "from pgmini.m2 import TabularFactor, UGraph\n",
        "from pgmini.m3 import TabularCPDFactor, PGM, BayesianNetwork, MarkovNetwork\n",
        "import functools\n",
        "import itertools\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from tabulate import tabulate\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c984fb82",
      "metadata": {
        "id": "c984fb82"
      },
      "source": [
        "# Introduction: Medical Diagnosis\n",
        "\n",
        "The example that we will reason about is one where we want can predict probability of bronchitis, based on other variables (smoking, influenza, couching, etc.).\n",
        "Note: The probabilities in this example do not necessarily reflect true probabilities in the real world.\n",
        "\n",
        "The BN is defined by the following conditional probabilities:\n",
        "\n",
        "* $P(\\verb+Influenza+ = 1) = 0.1$\n",
        "* $P(\\verb+Smokes+ = 1) = 0.3$\n",
        "* $P(\\verb+SoreThroat+ = 1 | \\verb+Influenza+ = 1) = 0.2$\n",
        "* $P(\\verb+SoreThroat+ = 1 | \\verb+Influenza+ = 0) = 0.01$\n",
        "* $P(\\verb+Fever+ = 1| \\verb+Influenza+ = 1) = 0.95$\n",
        "* $P(\\verb+Fever+ = 1| \\verb+Influenza+ = 0) = 0.1$\n",
        "* $P(\\verb+Bronchitis+ = 1 | \\verb+Influenza+ = 1, \\verb+Smokes+ = 1) = 0.99$\n",
        "* $P(\\verb+Bronchitis+ = 1 | \\verb+Influenza+ = 1, \\verb+Smokes+ = 0) = 0.95$\n",
        "* $P(\\verb+Bronchitis+ = 1 | \\verb+Influenza+ = 0, \\verb+Smokes+ = 1) = 0.75$\n",
        "* $P(\\verb+Bronchitis+ = 1 | \\verb+Influenza+ = 0, \\verb+Smokes+ = 0) = 0.0001$\n",
        "* $P(\\verb+Coughing+ = 1| \\verb+Bronchitis+ = 1) = 0.75$\n",
        "* $P(\\verb+Coughing+ = 1| \\verb+Bronchitis+ = 0) = 0.05$\n",
        "* $P(\\verb+Wheezing+ = 1| \\verb+Bronchitis+ = 1) = 0.5$\n",
        "* $P(\\verb+Wheezing+ = 1| \\verb+Bronchitis+ = 0) = 0.001$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa0ebf78",
      "metadata": {
        "id": "fa0ebf78"
      },
      "source": [
        "We can generate the factors similar to last weeks, but this time we are using a new type of object: TabularCPDFactor. This is essentially a TabularFactor, but its scope is made of `parents` and a `child`, and the table must form a valid CPD. This kind of object is convenient in the context of VE because VE builds largely on _factor operations_, which TabularFactor from the previous module supports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86567fd7",
      "metadata": {
        "id": "86567fd7"
      },
      "outputs": [],
      "source": [
        "medical_outcome_spaces = {\n",
        "    'I': OutcomeSpace(['i0', 'i1']),\n",
        "    'S': OutcomeSpace(['s0', 's1']),\n",
        "    'ST': OutcomeSpace(['st0', 'st1']),\n",
        "    'F': OutcomeSpace(['f0', 'f1']),\n",
        "    'B': OutcomeSpace(['b0', 'b1']),\n",
        "    'C': OutcomeSpace(['c0', 'c1']),\n",
        "    'W': OutcomeSpace(['w0', 'w1'])\n",
        "}\n",
        "\n",
        "# Factors\n",
        "phi_I = TabularCPDFactor([], 'I', medical_outcome_spaces, [0.9, 0.1])\n",
        "phi_S = TabularCPDFactor([], 'S', medical_outcome_spaces, [0.7, 0.3])\n",
        "phi_ST = TabularCPDFactor(['I'], 'ST', medical_outcome_spaces, [[0.99, 0.01], [0.8, 0.2]])\n",
        "phi_F = TabularCPDFactor(['I'], 'F', medical_outcome_spaces, [[0.9, 0.1], [0.05, 0.95]])\n",
        "phi_B = TabularCPDFactor(['I', 'S'], 'B', medical_outcome_spaces,\n",
        "[[[0.999, 0.001], [0.25, 0.75]], [[0.05, 0.95], [0.1, 0.9]]])\n",
        "phi_C = TabularCPDFactor(['B'], 'C', medical_outcome_spaces, [[0.95, 0.05], [0.25, 0.75]])\n",
        "phi_W = TabularCPDFactor(['B'], 'W', medical_outcome_spaces, [[0.999, 0.001], [0.5, 0.5]])\n",
        "###\n",
        "\n",
        "# Display factors as tables\n",
        "for i, phi in enumerate([phi_I, phi_S, phi_ST, phi_F, phi_B, phi_C, phi_W], 1):\n",
        "    print(phi.display(factor_name=f\"phi{i}\"))\n",
        "    print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e5cab26",
      "metadata": {
        "id": "8e5cab26"
      },
      "source": [
        "In `pgmini.m3` we provide you with two types of PGMs, namely BNs and MNs. You can construct one such PGM from the factors directly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f9844a4",
      "metadata": {
        "id": "6f9844a4"
      },
      "outputs": [],
      "source": [
        "medical_bn = BayesianNetwork([phi_I, phi_S, phi_ST, phi_F, phi_B, phi_C, phi_W])\n",
        "print(medical_bn.dag)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48827b01",
      "metadata": {
        "id": "48827b01"
      },
      "source": [
        "Lets loop through the joint assignments and plot them in a table:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e541391d",
      "metadata": {
        "id": "e541391d"
      },
      "outputs": [],
      "source": [
        "def display_full_table(pgm, rvs=None):\n",
        "    \"\"\"\n",
        "    pgm: an instance of PGM\n",
        "    rvs: optionally specify the order in which to list rvs in the table\n",
        "    \"\"\"\n",
        "    table = []\n",
        "    # rvs = ['I', 'S', 'ST', 'F', 'B', 'C', 'W']\n",
        "    if rvs is None:\n",
        "        rvs = list(pgm.iternodes())\n",
        "    for assignment in pgm.enumerate_joint_assignments(rvs):\n",
        "        table.append([assignment[rv] for rv in rvs] + [pgm.evaluate(assignment)])\n",
        "    print(tabulate(table, headers=rvs + ['~P']))\n",
        "    print(\"Sum\", sum(row[-1] for row in table))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6621a6e5-8f9a-46b2-8baf-dec730217e85",
      "metadata": {
        "id": "6621a6e5-8f9a-46b2-8baf-dec730217e85"
      },
      "outputs": [],
      "source": [
        "display_full_table(medical_bn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9f9acbc",
      "metadata": {
        "id": "f9f9acbc"
      },
      "source": [
        "## Naive Marginalization\n",
        "\n",
        "Now that we have the graph, we can compute marginal probabilities. The sum and product rules tell us that we need to sum over all possible values of all other variables:\n",
        "\n",
        "$$\n",
        "P(B) = \\sum_{I, S, ST, F, C, W} P(I, S, ST, F, B, C, W)\n",
        "$$\n",
        "\n",
        "Expanding the joint distribution using the BN factorization, we get:\n",
        "\n",
        "$$\n",
        "P(B) = \\sum_{I, S, ST, F, C, W} \\prod_{X} P(X \\mid \\text{pa}(X))\n",
        "$$\n",
        "\n",
        "where $X$ ranges over all variables in the network ($I, S, ST, F, B, C, W$) and $\\text{pa}(X)$ denotes the parents of $X$ in the graph.\n",
        "\n",
        "Note that each P() can be considered a factor with scope {X, Pa(X)}\n",
        "\n",
        "\n",
        "We implemented the naive approach below, make sure to understand the code s.t. you can answer questions about it later. Feel free to play with the code itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4476284f",
      "metadata": {
        "id": "4476284f"
      },
      "outputs": [],
      "source": [
        "def naive_exact_inference(query_rvs: set, evidence: dict, pgm: PGM, reduce_first=False, normalize=False, trace=None):\n",
        "    \"\"\"\n",
        "    Return the (unnormalized) distribution over Q given E=e.\n",
        "\n",
        "    In naive exact inference we build the complete joint factor before conditioning\n",
        "    on evidence, then marginalise the necessary variables.\n",
        "\n",
        "    A first optimisation we can consider is to reduce to evidence first, and only then\n",
        "    multiply all factors together.\n",
        "\n",
        "    query_rvs: the set Q\n",
        "    evidence: a dict representing the evidence assignment E=e\n",
        "        Q and E are disjoint\n",
        "    pgm: a PGM (a BN or an MN)\n",
        "    reduce_first: an optimisation whereby we reduce the factors to evidence\n",
        "        before taking their product\n",
        "    normalize: whether or not we normalize the result.\n",
        "    trace: used to log some information about complexity\n",
        "    \"\"\"\n",
        "    assert len(set(query_rvs) & evidence.keys()) == 0, \"Q and E should be disjoint\"\n",
        "\n",
        "    # Log scope of factors we start with\n",
        "    if trace is not None:\n",
        "        trace.extend([(None, factor.scope) for factor in pgm.iterfactors()])\n",
        "\n",
        "    if reduce_first:\n",
        "        # reduce all factors to evidence\n",
        "        factors = [factor.reduce(evidence) for factor in pgm.iterfactors()]\n",
        "        out = functools.reduce(lambda a, b: a.product(b), factors)\n",
        "        if trace is not None: # Log what we did\n",
        "            trace.append((out.scope))\n",
        "    else:\n",
        "        out = functools.reduce(lambda a, b: a.product(b), pgm.iterfactors())\n",
        "        if trace is not None: # Log what we did\n",
        "            trace.append((out.scope))\n",
        "        out = out.reduce(evidence)\n",
        "\n",
        "\n",
        "    all_rvs = set(pgm.iternodes())\n",
        "    rvs_to_marginalize = all_rvs - set(query_rvs) - evidence.keys()\n",
        "\n",
        "    for rv in rvs_to_marginalize:\n",
        "        out = out.marginalize(rv)\n",
        "\n",
        "    return out.normalize() if normalize else out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13570390",
      "metadata": {
        "id": "13570390"
      },
      "outputs": [],
      "source": [
        "# Example query\n",
        "\n",
        "trace = []\n",
        "print(f\"Reduce later: \\n{naive_exact_inference({'B'}, {'W': 'w0', 'C': 'c0'}, medical_bn, trace=trace, reduce_first=False, normalize=True)}\")\n",
        "print(f\"\\nScope of factor product: {trace[-1]}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac97c537",
      "metadata": {
        "id": "ac97c537"
      },
      "source": [
        "**EXERCISE - Naive marginalization**\n",
        "\n",
        "Make sure to run example queries for your answers. For each of the questions show how this can be seen from running the example queries (if applicable).\n",
        "\n",
        "\n",
        "**Question 1:** You have a friend that coughs and does not smoke, what is the chance of them having a fever? Report how you used the code\n",
        "\n",
        "ANSWER: ...\n",
        "\n",
        "\n",
        "The following questions concern memory requirement for computing $P(F|C=c1, S=s0)$ using this naive approach:\n",
        "\n",
        "**Question 2:** Given reduction-later version, what is the scope and scope size of the factor product?\n",
        "\n",
        "ANSWER: ...\n",
        "\n",
        "\n",
        "\n",
        "**Question 3** Given the reduction-first version, what is the scope and scope size of the factor product?\n",
        "\n",
        "ANSWER: ...\n",
        "\n",
        "\n",
        "\n",
        "**Question 4** For each method, what is the size of its biggest factor table (variables are binary)?\n",
        "\n",
        "ANSWER: ...\n",
        "\n",
        "\n",
        "\n",
        "**Question 5** As we add more (binary) variables what happens to the memory requirement (max table size)? Discuss the big-O complexity in your answer for each of the methods. You can refer to the size of query rvs as n and the size of the evidence set as e.\n",
        "\n",
        "ANSWER: ...\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bdd9053",
      "metadata": {
        "id": "0bdd9053"
      },
      "outputs": [],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "# Make sure to run the queries that you use to answer the questions above."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe92147a",
      "metadata": {
        "id": "fe92147a"
      },
      "source": [
        "# Basic Variable Elimination (VE) for Sum-Product Inference\n",
        "\n",
        "VE attempts to tackle the exponential increase of the tabular view of the entire joint distribution.\n",
        "\n",
        "The core of the algorithm is noticing that we can move the products and sums around to reduce the sizes of intermediate tables (hence: variable-elimination). Based on a given order of variables, we combine the factors for which that variable is in the scope (using factor product), and then marginalize out that variable.\n",
        "\n",
        "Below we provide you with the code. Make sure to read the code so that you can understand and reason about it later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53aec540-8820-4585-9250-bd6771e00cfa",
      "metadata": {
        "id": "53aec540-8820-4585-9250-bd6771e00cfa"
      },
      "outputs": [],
      "source": [
        "def split_factors(rv, all_factors):\n",
        "    \"\"\"\n",
        "    Splits all_factors into a list that's relevant to the rv and another that's irrelevant.\n",
        "    A factor is \"relevant\" to an rv if that rv is in the factor's scope.\n",
        "    \"\"\"\n",
        "    relevant, irrelevant = [], []\n",
        "    for factor in all_factors:\n",
        "        if rv in factor:\n",
        "            relevant.append(factor)\n",
        "        else:\n",
        "            irrelevant.append(factor)\n",
        "    return relevant, irrelevant\n",
        "\n",
        "def random_order(node):\n",
        "    \"\"\"\n",
        "    Assign a random number to the node.\n",
        "    This will lead to a random order of elimination\n",
        "    \"\"\"\n",
        "    return random.random()\n",
        "\n",
        "\n",
        "def variable_elimination(query_rvs: set, evidence: dict, pgm: PGM, key=None, normalize=False, trace=None):\n",
        "    \"\"\"\n",
        "    Return a factor representation of the (unnormalised) distribution over Q given E=e.\n",
        "\n",
        "    query_rvs: the set Q\n",
        "    evidence: a dict representing the evidence assignment E=e\n",
        "        Q and E are disjoint\n",
        "    factors: a PGM (a BN or an MN)\n",
        "    key: use to specify the order of elimination\n",
        "        (this is passed to python's sorted function)\n",
        "    trace: if provided, we log here each variable that was eliminated, in order, and the scope\n",
        "        of the factor from which we eliminated it\n",
        "    \"\"\"\n",
        "    assert len(set(query_rvs) & evidence.keys()) == 0, \"Q and E should be disjoint\"\n",
        "\n",
        "    if isinstance(pgm, BayesianNetwork):  # we need to moralise BNs\n",
        "        pgm = MarkovNetwork(pgm.iterfactors())\n",
        "\n",
        "    # Log scope of factors we start with\n",
        "    if trace is not None:\n",
        "        trace.extend([(None, factor.scope) for factor in pgm.iterfactors()])\n",
        "\n",
        "    # reduce all factors using the available evidence\n",
        "    factors = [factor.reduce(evidence) for factor in pgm.iterfactors()]\n",
        "    # rvs that need to be eliminated/marginalised\n",
        "    all_rvs = set(pgm.iternodes())\n",
        "    rvs_to_marginalize = all_rvs - set(query_rvs) - evidence.keys()\n",
        "\n",
        "    # eliminate each rv in order\n",
        "    for rv in sorted(rvs_to_marginalize, key=key):\n",
        "        # separate factors containing this rv in their scope\n",
        "        involved, factors = split_factors(rv, factors)\n",
        "        if not involved:  # nothing to do here\n",
        "            continue\n",
        "        # take the product of the factors involved in this elimination step\n",
        "        new_factor = functools.reduce(lambda a, b: a.product(b), involved)\n",
        "\n",
        "        # Log intermediate factor products\n",
        "        if trace is not None:\n",
        "            trace.append((rv, new_factor.scope))\n",
        "\n",
        "        # marginalise / eliminate the rv from the factor product\n",
        "        new_factor = new_factor.marginalize({rv})\n",
        "        # keep the factor for future use\n",
        "        factors.append(new_factor)\n",
        "\n",
        "    # multiply whatever factors remain\n",
        "    out = functools.reduce(lambda a, b: a.product(b), factors)\n",
        "    return out.normalize() if normalize else out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "611db96e-6d91-4e45-b005-ed30d670f07d",
      "metadata": {
        "id": "611db96e-6d91-4e45-b005-ed30d670f07d"
      },
      "source": [
        "This is an example order of elimiantion:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b11fefc9-51b7-4444-a665-f5fee2474556",
      "metadata": {
        "id": "b11fefc9-51b7-4444-a665-f5fee2474556"
      },
      "outputs": [],
      "source": [
        "for node in sorted(medical_bn.iternodes(), key=random_order):\n",
        "    print(node)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cb5a4fc",
      "metadata": {
        "id": "5cb5a4fc"
      },
      "source": [
        "We provide some example runs of VE:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fe2e8e0",
      "metadata": {
        "id": "9fe2e8e0"
      },
      "outputs": [],
      "source": [
        "print(variable_elimination({'F'}, dict(), medical_bn).normalize())\n",
        "\n",
        "trace = []\n",
        "print(variable_elimination({'B'}, {'W': 'w0', 'C': 'c0'}, medical_bn, trace=trace).normalize())\n",
        "print(tabulate(trace, headers=['sum', 'scope']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "608b0165",
      "metadata": {
        "id": "608b0165"
      },
      "source": [
        "Below, we provide two helpers for the kind of information you will need in the following exercise. Read the exercise first, then complete the helpers. _Remark:_ if you find it difficult to work with our helpers, it is okay to create your own plotting code, but then we expect the plot to be as clear and as informative as ours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e647a9c",
      "metadata": {
        "id": "7e647a9c"
      },
      "outputs": [],
      "source": [
        "def get_max_factor_scope_size(pgm, rv, evidence = dict(), elimination_algorithm=variable_elimination, key=None):\n",
        "    \"\"\"\n",
        "    Get the maximal factor scope size for a given PGM, rv, evidence, elimination algorithm and key.\n",
        "    \"\"\"\n",
        "\n",
        "    raise NotImplementedError(\"Implementing this is an exercise\")\n",
        "\n",
        "\n",
        "\n",
        "def plot_max_factor_scope_sizes(pgm, num_runs=1, evidence = dict(), elimination_algorithm=variable_elimination, key=random_order):\n",
        "    \"\"\"\n",
        "    Plot the histogram of the maximal factor scope sizes for many random orders.\n",
        "    \"\"\"\n",
        "\n",
        "    max_scopes = []\n",
        "\n",
        "    for rv in pgm.iternodes():\n",
        "        for _ in range(num_runs):\n",
        "            # Check if the rv is in the evidence\n",
        "            if rv in evidence:\n",
        "                continue\n",
        "            max_scope = get_max_factor_scope_size(pgm, rv, evidence, elimination_algorithm, key)\n",
        "            max_scopes.append(max_scope)\n",
        "\n",
        "    # plot the histogram with number in middle of bin\n",
        "    plt.hist(max_scopes, bins=range(1, max(max_scopes) + 2), align='left', edgecolor='black')\n",
        "    # Add a vertical line at the scope size of the naive approach\n",
        "    plt.axvline(x=len(pgm.dag.nodes), color='red', linestyle='--', label='Naive Approach')\n",
        "    plt.title('Histogram of Maximal Factor Scope Sizes ')\n",
        "    plt.xlabel('Maximal Factor Scope Size')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ba11b98",
      "metadata": {
        "id": "7ba11b98"
      },
      "outputs": [],
      "source": [
        "assert get_max_factor_scope_size(medical_bn, 'I', dict(), variable_elimination) == 5\n",
        "assert get_max_factor_scope_size(medical_bn, 'B', dict(), variable_elimination) == 4\n",
        "assert get_max_factor_scope_size(medical_bn, 'F', dict(), variable_elimination) == 5\n",
        "assert get_max_factor_scope_size(medical_bn, 'C', dict(), variable_elimination) == 5\n",
        "assert get_max_factor_scope_size(medical_bn, 'W', dict(), variable_elimination) == 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da83640d",
      "metadata": {
        "id": "da83640d"
      },
      "outputs": [],
      "source": [
        "# Use this cell to generate the plot for the next exercise\n",
        "\n",
        "# **Solution**\n",
        "\n",
        "\n",
        "# **End of Solution**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26636ce7",
      "metadata": {
        "id": "26636ce7"
      },
      "source": [
        "### EXERCISE - Basic VE ###\n",
        "In the above examples we computed probabilties in a way that is more efficient than the naive implementation that we started with. But how much more efficient? Run the algorithm multiple times, using random elimination orderings, in order to compute all _single-variable marginal probability queries_ (P(I), P(S), P(ST), etc.). Find the size of the scope of the biggest factor product for each run and plot them in a histogram. Make sure to use random orderings when running the algorithm multiple times. To plot the histogram you must first implement the helper function.\n",
        "\n",
        "Specifically answer the following question:\n",
        "\n",
        "**Question 1**: What are the sizes of the biggest (intermediate) factor(product)s? Report what you see in the plots\n",
        "\n",
        "ANSWER: ...\n",
        "\n",
        "\n",
        "**Question 2**: How does this compare to the size of the biggest (intermediate) factor(product) in the naive case?\n",
        "\n",
        "ANSWER: ...\n",
        "\n",
        "**Question 3**: How come VE never produces as big factor products as the naive case, regardless of which ordering we choose? Refer to the operations in the code in your answer.\n",
        "\n",
        "ANSWER: ...\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "669056bf",
      "metadata": {
        "id": "669056bf"
      },
      "source": [
        "# Graphical Seperation #\n",
        "\n",
        "\n",
        "In the lectures we have seen that we can further optimize the VE algorithm by using seperation structure in the graph.\n",
        "\n",
        "If an rv Y to be eliminated is separate from Q given E, then we can eliminate Y without actually manipulating the factors relevant to it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c52d5c58-9ce5-4f8d-9a1b-2cd564c3c328",
      "metadata": {
        "id": "c52d5c58-9ce5-4f8d-9a1b-2cd564c3c328"
      },
      "source": [
        "### EXERCISE - Graphical Seperation - Part I ###\n",
        "\n",
        "In this exercise you have to implement `variable_elimination_with_separation`, which modifies `variable_elimination` to make use of graphical separation in an attempt to spare unnecessary computation.\n",
        "\n",
        "Note that PGM has a method `separate(X, Y, Z)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c06801c",
      "metadata": {
        "id": "5c06801c"
      },
      "outputs": [],
      "source": [
        "def variable_elimination_with_separation(query_rvs: set, evidence: dict, pgm: PGM, key=None, normalize=False, trace=None):\n",
        "    \"\"\"\n",
        "    Return a factor representation of the (unnormalised) distribution over Q given E=e.\n",
        "\n",
        "    query_rvs: the set Q\n",
        "    evidence: a dict representing the evidence assignment E=e\n",
        "        Q and E are disjoint\n",
        "    factors: a PGM (a BN or an MN)\n",
        "    key: use to specify the order of elimination\n",
        "        (this is passed to python's sorted function)\n",
        "    normalize: whether or not we normalize the result.\n",
        "    trace: if provided, we log here each variable that was eliminated, in order, and the scope\n",
        "        of the factor from which we eliminated it\n",
        "    \"\"\"\n",
        "    assert len(set(query_rvs) & evidence.keys()) == 0, \"Q and E should be disjoint\"\n",
        "\n",
        "\n",
        "    if isinstance(pgm, BayesianNetwork):  # we moralise the BN\n",
        "        pgm = MarkovNetwork([factor for factor in pgm.iterfactors()])\n",
        "    # Log scope of factors we start with\n",
        "    if trace is not None:\n",
        "        trace.extend([(None, factor.scope) for factor in pgm.iterfactors()])\n",
        "\n",
        "    # reduce all factors using the available evidence\n",
        "    factors = [factor.reduce(evidence) for factor in pgm.iterfactors()]\n",
        "    # rvs that need to be eliminated/marginalised\n",
        "    all_rvs = set(pgm.iternodes())\n",
        "    Q = set(query_rvs)\n",
        "    E = set(evidence.keys())\n",
        "    rvs_to_marginalize = all_rvs - Q - E\n",
        "\n",
        "\n",
        "    # eliminate each rv in order\n",
        "    for rv in sorted(rvs_to_marginalize, key=key):\n",
        "        # separate factors containing this rv in their scope\n",
        "        involved, factors = split_factors(rv, factors)\n",
        "        if not involved:  # nothing to do here\n",
        "            if trace is not None:  # possibly log what we did\n",
        "                trace.append((rv, tuple()))\n",
        "            continue\n",
        "\n",
        "        # **SOLUTION**\n",
        "\n",
        "\n",
        "        # **END OF SOLUTION**\n",
        "\n",
        "        # take the product of the factors involved in this elimination step\n",
        "        new_factor = functools.reduce(lambda a, b: a.product(b), involved)\n",
        "        if trace is not None:  # possibly log what we did\n",
        "            trace.append((rv, new_factor.scope))\n",
        "        # marginalise / eliminate the rv from the factor product\n",
        "        new_factor = new_factor.marginalize({rv})\n",
        "        # keep the factor for future use\n",
        "        factors.append(new_factor)\n",
        "\n",
        "    # multiply whatever factors remain\n",
        "    out = functools.reduce(lambda a, b: a.product(b), factors)\n",
        "    return out.normalize() if normalize else out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dc4b735",
      "metadata": {
        "id": "4dc4b735"
      },
      "source": [
        "Some checks the code works as it should"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3467dc0",
      "metadata": {
        "id": "b3467dc0"
      },
      "outputs": [],
      "source": [
        "trace = []\n",
        "variable_elimination_with_separation(query_rvs = {'F'}, evidence = {'I':'i0'}, pgm = medical_bn, trace=trace).normalize()\n",
        "assert trace[-1][1] == ()\n",
        "variable_elimination_with_separation(query_rvs = {'B'}, evidence = {'I':'i0'}, pgm = medical_bn, trace=trace).normalize()\n",
        "assert trace[-1][1] == ('B', 'W')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e3eef93",
      "metadata": {
        "id": "8e3eef93"
      },
      "source": [
        "This new algorithm should perform better when variables are seperated from the rest of the graph, given the evidence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b63f5fd",
      "metadata": {
        "id": "2b63f5fd"
      },
      "outputs": [],
      "source": [
        "trace = []\n",
        "# print(variable_elimination(query_rvs = {'F'}, evidence = {'I':'i0'}, pgm = medical_bn, trace=trace).normalize())\n",
        "variable_elimination(query_rvs = {'F'}, evidence = {'I':'i0'}, pgm = medical_bn, trace=trace).normalize()\n",
        "print(tabulate(trace, headers=['sum', 'scope']))\n",
        "\n",
        "\n",
        "trace = []\n",
        "# print(variable_elimination_with_separation(query_rvs = {'F'}, evidence = {'I':'i0'}, pgm = medical_bn, trace=trace).normalize())\n",
        "variable_elimination_with_separation(query_rvs = {'F'}, evidence = {'I':'i0'}, pgm = medical_bn, trace=trace).normalize()\n",
        "# We should see an absence of factor products as F is seperate from the rest of the graph given the evidence\n",
        "print(tabulate(trace, headers=['sum', 'scope']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "773dac17",
      "metadata": {
        "id": "773dac17"
      },
      "outputs": [],
      "source": [
        "# Before coding the following cell, make sure to read the exercise below it.\n",
        "\n",
        "# Add plots here that show the difference in max factor scope size between the two algorithms\n",
        "# You should use the plotting function from the previous exercise\n",
        "# Use a high number of runs to get a good estimate\n",
        "\n",
        "# **Solution**\n",
        "\n",
        "\n",
        "# **End of Solution**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97f31b26",
      "metadata": {
        "id": "97f31b26"
      },
      "source": [
        "### EXERCISE - Graphical Seperation - Part II ###\n",
        "\n",
        "\n",
        "**Question 1** For which variable outcomes in the evidence do the algorithms give different max factor scope size? Run the plotting function for those variables as part of your answer.\n",
        "\n",
        "ANSWER: ...\n",
        "\n",
        "\n",
        "**Question 2** What do you observe w.r.t. maximal factor scope size for queries using this evidence?\n",
        "\n",
        "ANSWER: ...\n",
        "\n",
        "**Question 3** Why do these queries result in smaller max factor scope?\n",
        "\n",
        "ANSWER: ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d428d3d-e345-462e-85a3-6f69ff55e1ed",
      "metadata": {
        "id": "2d428d3d-e345-462e-85a3-6f69ff55e1ed"
      },
      "source": [
        "# Elimination Order Heuristics\n",
        "\n",
        "\n",
        "We have seen that the VE algorithm always outperforms the naive approach, but the different orders produce different sizes of the biggest factor product. For small graphs like the one in this example that is fine, but for bigger graphs this can become problematic easily. In the following part of the notebook we will look at different deterministic orderings. A small note here is that finding the optimal ordering is NP-complete. What we can do however, is define heurstics based on graph properties.\n",
        "\n",
        "The general idea is to code a function that could be used as `key` for python's `sorted`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf0ddbce",
      "metadata": {
        "id": "cf0ddbce"
      },
      "outputs": [],
      "source": [
        "def min_lexicographic(node):\n",
        "    \"\"\"Return the lexicographically smallest node\"\"\"\n",
        "    return str(node)\n",
        "\n",
        "for node in sorted(medical_bn.iternodes(), key=min_lexicographic):\n",
        "    print(node)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd34c9ba",
      "metadata": {
        "id": "cd34c9ba"
      },
      "outputs": [],
      "source": [
        "trace = []\n",
        "print(variable_elimination({'B'}, dict(), medical_bn, trace=trace, key=min_lexicographic).normalize())\n",
        "print(tabulate(trace, headers=['sum', 'scope']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f54d6de1",
      "metadata": {
        "id": "f54d6de1"
      },
      "outputs": [],
      "source": [
        "# plot_max_factor_scope_sizes(medical_bn, key=min_lexicographic)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "072e4c47",
      "metadata": {
        "id": "072e4c47"
      },
      "source": [
        "### Exercise - Order heuristics - Part I ###\n",
        "Below we have multiple functions that are not implemented yet. Implement the functions and test them. Make sure to draw out the graph by hand to verify your answer.\n",
        "\n",
        "To be able to make this function specific to a given PGM, we define it with a `pgm` as argument, and then bind it to a specific PGM object when calling `variable_elimination` using `functools.partial`.\n",
        "\n",
        "Also, make sure to break ties lexographically. For this it can be helpful to know that sorted() can take a tuple as input, where the second value is used to solve tiebreaks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f295411-7675-4eeb-a5e4-8acc04a5d0a1",
      "metadata": {
        "id": "7f295411-7675-4eeb-a5e4-8acc04a5d0a1"
      },
      "outputs": [],
      "source": [
        "def min_cardinality(node, pgm: PGM):\n",
        "    \"\"\"Return the cardinality of the rv in the PGM\"\"\"\n",
        "\n",
        "    raise NotImplementedError(\"Implementing this is an exercise\")\n",
        "\n",
        "\n",
        "def min_neighbors(node, pgm: PGM):\n",
        "    \"\"\"Return the number of neighbours of the rv in the underlying graph\"\"\"\n",
        "\n",
        "    raise NotImplementedError(\"Implementing this is an exercise\")\n",
        "\n",
        "\n",
        "def min_degree(node, pgm: PGM):\n",
        "    \"\"\"Return number of factors whose scope contains the rv\"\"\"\n",
        "\n",
        "    raise NotImplementedError(\"Implementing this is an exercise\")\n",
        "\n",
        "\n",
        "def min_weight(node, pgm: PGM):\n",
        "    \"\"\"\n",
        "    Find the set of all rvs connected by factor\n",
        "    return the product of their cardinalities.\n",
        "    \"\"\"\n",
        "\n",
        "    raise NotImplementedError(\"Implementing this is an exercise\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18025686",
      "metadata": {
        "id": "18025686"
      },
      "outputs": [],
      "source": [
        "for node in medical_bn.iternodes():\n",
        "    card = neighbors = degree = weight = \"Not implemented\"\n",
        "#     card = min_cardinality(node, medical_bn)\n",
        "#     neighbors = min_neighbors(node, medical_bn)\n",
        "#     degree = min_degree(node, medical_bn)\n",
        "#     weight = min_weight(node, medical_bn)\n",
        "    print(f\"  {node}: cardinality={card}, neighbors={neighbors}, degree={degree}, weight={weight}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "973e33a2",
      "metadata": {
        "id": "973e33a2"
      },
      "outputs": [],
      "source": [
        "# Use this cell to run the code for your answers for the exercise below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a7095bc",
      "metadata": {
        "id": "2a7095bc"
      },
      "source": [
        "### EXERCISE - Ordering Heuristics - Part II ###\n",
        "\n",
        "Report the results you observe for P(B),\n",
        "\n",
        "***Question 1***: Which ordering functions produce the smallest maximal scope size? (report the maximal scope size in your answer and leave the code that you used to get the answer in the notebook)\n",
        "\n",
        "ANSWER: ...\n",
        "\n",
        "***Question 2*** Which ordering functions are least effective, why? (report the maximal scope size in your answer)\n",
        "\n",
        "ANSWER: ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89e3b2e5",
      "metadata": {
        "id": "89e3b2e5"
      },
      "source": [
        "# Bonus - Differentiate between ordering functions #\n",
        "\n",
        "In the previous assignment you have seen that multiple ordering functions produce the same optimal maximal scope size. This has to do with the fact that we have a small example graph of a specific form for which many ordering functions give an optimal ordering.\n",
        "\n",
        "For this bonus exercise we invite you to build a small graph of 4 variables that gives a different performance for at least one of the three optimal ordering functions. You are free to choose both the edges and the cardinality of the variables (Also, remember that tie-breaks should be implemented lexographically).\n",
        "\n",
        "\n",
        "The last cell of this notebook contains the test for your graph.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdf11656",
      "metadata": {
        "id": "fdf11656"
      },
      "outputs": [],
      "source": [
        "# Define your graph here\n",
        "\n",
        "# bonus_outcome_spaces = ...\n",
        "\n",
        "# phi_A = ...\n",
        "# phi_B = ...\n",
        "# phi_C = ...\n",
        "# phi_D = ...\n",
        "\n",
        "# bonus_pgm = ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28222d49",
      "metadata": {
        "id": "28222d49"
      },
      "outputs": [],
      "source": [
        "# Run the order heuristics for the graph in this cell\n",
        "\n",
        "# Test algorithms on P(C) and compare ordering functions\n",
        "query_var = 'C' # Or any other variable of your choice\n",
        "\n",
        "# Test each ordering\n",
        "orderings_to_test = [\n",
        "    ('min_lexicographic', lambda pgm: min_lexicographic),\n",
        "    ('min_cardinality', lambda pgm: functools.partial(min_cardinality, pgm=pgm)),\n",
        "    ('min_neighbors', lambda pgm: functools.partial(min_neighbors, pgm=pgm)),\n",
        "    ('min_degree', lambda pgm: functools.partial(min_degree, pgm=pgm)),\n",
        "    ('min_weight', lambda pgm: functools.partial(min_weight, pgm=pgm))\n",
        "]\n",
        "\n",
        "results = {}\n",
        "\n",
        "for ordering_name, key_maker in orderings_to_test:\n",
        "    key_func = key_maker(bonus_pgm)\n",
        "    results[ordering_name] = get_max_factor_scope_size(bonus_pgm, query_var, dict(), variable_elimination, key_func)\n",
        "\n",
        "# Display results\n",
        "print(f\"Max Scope Sizes on P({query_var}):\")\n",
        "print(tabulate([\n",
        "    [ordering, results[ordering]] for ordering, _ in orderings_to_test\n",
        "], headers=['Ordering', 'Max Scope Size']))\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "024dce6a",
      "metadata": {
        "id": "024dce6a"
      },
      "source": [
        "# Use of AI Tools\n",
        "\n",
        "By submitting this notebook for grading you testify that:\n",
        "\n",
        "* AI did not draft an earlier version of your work.\n",
        "* You did not use AI-powered code completion.\n",
        "* You did not implement algorithms suggested by an AI tool.\n",
        "* AI did not revise a version of your work.\n",
        "* You did not implement suggestions made by an AI tool.\n",
        "\n",
        "\n",
        "_You_ in the sentences above refers to you and all your team members.\n",
        "_AI_ refers to LM-based tools and assistants (e.g., ChatGPT, Gemini, UvA AI chat, etc.).\n",
        "\n",
        "If you did make use of an AI tool, you should describe the uses you made of it below. Or indicate that no such tool was used."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ac9e01d",
      "metadata": {
        "id": "2ac9e01d"
      },
      "source": [
        "**TYPE YOUR STATEMENT HERE**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}